{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5cb84e8",
   "metadata": {},
   "source": [
    "### Nous allons ici faire quelques applications d'augmentation des données textuelles en suivant un tutoriel sur medium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73361097",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nlpaug\n",
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4c2112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rosalie/miniforge3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a1ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Un exemple de texte qu'on veut reformuler \n",
    "text = \"The quick brown fox jumps over a lazy dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b61473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nltk\n",
    "\n",
    "# 1) Dossier local et inscriptible pour les données NLTK\n",
    "NLTK_DIR = os.path.expanduser(\"~/nltk_data\")\n",
    "os.makedirs(NLTK_DIR, exist_ok=True)\n",
    "if NLTK_DIR not in nltk.data.path:\n",
    "    nltk.data.path.append(NLTK_DIR)\n",
    "\n",
    "# 2) Téléchargements nécessaires\n",
    "# - wordnet + omw-1.4 : pour les synonymes\n",
    "# - averaged_perceptron_tagger_eng : POS tagger (NLTK 3.8+)\n",
    "# - averaged_perceptron_tagger : par compatibilité descendante (certaines libs le demandent encore)\n",
    "# - punkt : tokenisation de base (utile selon les tokenizers)\n",
    "for pkg in [\"wordnet\", \"omw-1.4\", \"averaged_perceptron_tagger_eng\",\n",
    "            \"averaged_perceptron_tagger\", \"punkt\"]:\n",
    "    try:\n",
    "        nltk.download(pkg, download_dir=NLTK_DIR, quiet=True)\n",
    "    except Exception as e:\n",
    "        print(f\"NLTK download failed for {pkg}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8ec41e",
   "metadata": {},
   "source": [
    "#### Synonym Replacement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "831efad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym Text:  ['The quick robert brown fox bound over a lazy wienerwurst']\n"
     ]
    }
   ],
   "source": [
    "syn_aug = naw.synonym.SynonymAug(aug_src=\"wordnet\")\n",
    "synonym_text = syn_aug.augment(text)\n",
    "print(\"Synonym Text: \", synonym_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4eb4bc",
   "metadata": {},
   "source": [
    "#### Random Substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a89133a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substituted Text:  ['_ quick brown _ jumps over a lazy _']\n"
     ]
    }
   ],
   "source": [
    "sub_aug = naw.random.RandomWordAug(action='substitute')\n",
    "substituted_text = sub_aug.augment(text)\n",
    "print(\"Substituted Text: \", substituted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c34e66",
   "metadata": {},
   "source": [
    "### Random Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad081b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deletion Text:  ['The jumps over a lazy dog']\n"
     ]
    }
   ],
   "source": [
    "del_aug = naw.random.RandomWordAug(action='delete')\n",
    "deletion_text = del_aug.augment(text)\n",
    "print(\"Deletion Text: \", deletion_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f062258d",
   "metadata": {},
   "source": [
    "### Random Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb44cd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swap Text:  ['Quick the brown jumps fox over lazy a dog']\n"
     ]
    }
   ],
   "source": [
    "swap_aug = naw.random.RandomWordAug(action='swap')\n",
    "swap_text = swap_aug.augment(text)\n",
    "print(\"Swap Text: \", swap_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9decddfb",
   "metadata": {},
   "source": [
    "### Back Translation\n",
    "\n",
    "Translate original text to other language (like french) and convert back to english language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a95e1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back Translated Text:  ['The speedy brown fox jumps over a lazy dog']\n"
     ]
    }
   ],
   "source": [
    "back_trans_aug = naw.back_translation.BackTranslationAug()\n",
    "back_trans_text = back_trans_aug.augment(text)\n",
    "print(\"Back Translated Text: \", back_trans_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff9bda2",
   "metadata": {},
   "source": [
    "### Nous allons appliquer la Rétrotraduction pour former notre premier jeu de données augmenté \n",
    "Nous allons appliquer cela sur les données de texte brute ensuite on fera encore le nettoyage, nous allons appliquer l'augmentation uniquement pour les articles de type VS qui est sous représenté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b75e257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nltk\n",
    "\n",
    "# Dossier local pour les données NLTK (avec droits d’écriture)\n",
    "NLTK_DIR = os.path.expanduser(\"~/nltk_data\")\n",
    "os.makedirs(NLTK_DIR, exist_ok=True)\n",
    "if NLTK_DIR not in nltk.data.path:\n",
    "    nltk.data.path.append(NLTK_DIR)\n",
    "\n",
    "# Paquets requis (NLTK 3.8+)\n",
    "for pkg in [\n",
    "    \"punkt\",                         # tokeniseur historique\n",
    "    \"punkt_tab\",                     # depuis NLTK 3.8\n",
    "    \"stopwords\",\n",
    "    \"wordnet\", \"omw-1.4\",\n",
    "    \"averaged_perceptron_tagger_eng\",\n",
    "    \"averaged_perceptron_tagger\",    # compat descente\n",
    "]:\n",
    "    try:\n",
    "        nltk.download(pkg, download_dir=NLTK_DIR, quiet=True)\n",
    "    except Exception as e:\n",
    "        print(f\"NLTK download failed for {pkg}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "from typing import Optional, List\n",
    "import torch, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# ==== Init NLTK stuff ====\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "custom_stopwords = {'et', 'al'}\n",
    "\n",
    "# ==== Utils ====\n",
    "def coerce_to_str(x) -> str:\n",
    "    if isinstance(x, list):\n",
    "        x = \" \".join(map(str, x))\n",
    "    elif pd.isna(x):\n",
    "        x = \"\"\n",
    "    return str(x).strip()\n",
    "\n",
    "def safe_augment(aug, text: str, n: int = 1) -> List[str]:\n",
    "    \"\"\"Retourne une liste de n paraphrases (peut être < n si le modèle échoue).\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    try:\n",
    "        out = aug.augment(text, n=n)  # peut renvoyer str ou list[str]\n",
    "        if isinstance(out, str):\n",
    "            out = [out]\n",
    "        return [t.strip() for t in out if isinstance(t, str) and t.strip()]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def nettoyer_texte_tokens(texte: str) -> List[str]:\n",
    "    tokens = word_tokenize(texte)\n",
    "    tokens_nettoyes = []\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        token = re.sub(r'\\s+', '', token)\n",
    "        token = re.sub(r'[^a-zàâçéèêëîïôûùüÿñæœ]', '', token)\n",
    "        if token and token not in stop_words and token not in punctuation and token not in custom_stopwords:\n",
    "            token = lemmatizer.lemmatize(token)\n",
    "            tokens_nettoyes.append(token)\n",
    "    return tokens_nettoyes\n",
    "\n",
    "def tokens_to_text(tokens: List[str]) -> str:\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# ==== Device & models ====\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device choisi :\", device)\n",
    "\n",
    "from_model, to_model = (\n",
    "    (\"facebook/wmt19-en-de\", \"facebook/wmt19-de-en\") if device == \"cuda\"\n",
    "    else (\"Helsinki-NLP/opus-mt-en-de\", \"Helsinki-NLP/opus-mt-de-en\")\n",
    ")\n",
    "\n",
    "back_trans_aug = naw.BackTranslationAug(\n",
    "    from_model_name=from_model,\n",
    "    to_model_name=to_model,\n",
    "    device=device,\n",
    "    batch_size=32 if device == \"cuda\" else 8,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# ==== Load data ====\n",
    "df = pd.read_csv(\"./data/data_final_phase2_private.csv\")\n",
    "df[\"text\"] = df[\"text\"].apply(coerce_to_str)\n",
    "\n",
    "# (Re)crée text_clean / token_clean si manquants\n",
    "if \"token_clean\" not in df.columns:\n",
    "    df[\"token_clean\"] = df[\"text\"].apply(nettoyer_texte_tokens)\n",
    "else:\n",
    "    mask = df[\"token_clean\"].isna()\n",
    "    df.loc[mask, \"token_clean\"] = df.loc[mask, \"text\"].apply(nettoyer_texte_tokens)\n",
    "\n",
    "if \"text_clean\" not in df.columns:\n",
    "    df[\"text_clean\"] = df[\"token_clean\"].apply(tokens_to_text)\n",
    "else:\n",
    "    mask = df[\"text_clean\"].isna()\n",
    "    df.loc[mask, \"text_clean\"] = df.loc[mask, \"token_clean\"].apply(tokens_to_text)\n",
    "\n",
    "# ==== Filtrer uniquement la classe VS ====\n",
    "# (Robuste si tes labels sont 'VS'/'NVS' en str; adapte si 1/0)\n",
    "mask_vs = df[\"type_article\"].astype(str).str.upper().eq(\"VS\")\n",
    "df_vs = df[mask_vs].copy()\n",
    "\n",
    "# === Paramètre: nombre d'augmentations par article VS ===\n",
    "n_aug_per_sample = 1   # mets 2, 3… pour plus de paraphrases par texte VS\n",
    "\n",
    "aug_rows = []\n",
    "for _, row in df_vs.iterrows():\n",
    "    raw = row[\"text\"]\n",
    "    aug_texts = safe_augment(back_trans_aug, raw[:2000], n=n_aug_per_sample)\n",
    "    for aug_text in aug_texts:\n",
    "        tokens_bt = nettoyer_texte_tokens(aug_text)\n",
    "        aug_rows.append({\n",
    "            \"text_src\": raw,\n",
    "            \"text_final\": aug_text,\n",
    "            \"source\": \"bt\",\n",
    "            \"token_clean\": tokens_bt,\n",
    "            \"text_clean\": tokens_to_text(tokens_bt),\n",
    "            \"type_article\": row[\"type_article\"],\n",
    "            \"thematique\": row.get(\"thematique\", \"\")\n",
    "        })\n",
    "\n",
    "aug_df = pd.DataFrame(aug_rows)\n",
    "\n",
    "# ==== Bloc original (toutes les classes) ====\n",
    "train_original = df.copy()\n",
    "train_original[\"text_src\"] = train_original[\"text\"]\n",
    "train_original[\"text_final\"] = train_original[\"text\"]\n",
    "train_original[\"source\"] = \"orig\"\n",
    "\n",
    "# ==== Harmonisation & concat ====\n",
    "cols = [\"text_final\", \"text_clean\", \"token_clean\", \"source\", \"type_article\", \"thematique\", \"text_src\"]\n",
    "\n",
    "def ensure_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        return x.split()\n",
    "    return []\n",
    "\n",
    "train_original = train_original.reindex(columns=cols, fill_value=\"\")\n",
    "train_original[\"token_clean\"] = train_original[\"token_clean\"].apply(ensure_list)\n",
    "train_original[\"text_clean\"] = train_original[\"text_clean\"].astype(str)\n",
    "\n",
    "train_aug = aug_df.reindex(columns=cols, fill_value=\"\")\n",
    "\n",
    "train_data = pd.concat([train_original, train_aug], ignore_index=True)\n",
    "\n",
    "print(\"Nb lignes original :\", len(df))\n",
    "print(\"Nb VS augmentées   :\", len(aug_df))\n",
    "print(\"Taille finale       :\", train_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2af5880",
   "metadata": {},
   "source": [
    "#### Essayons ici de rattraper le nombre d'article de NVS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f76dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device choisi : cuda\n",
      "NVS: 2241  VS: 249\n",
      "Chaque article VS doit produire à peu près 9 versions (dont l'original).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "from typing import Optional, List\n",
    "import torch, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "import os\n",
    "import math\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# ==== Init NLTK stuff ====\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "custom_stopwords = {'et', 'al'}\n",
    "\n",
    "# ==== Utils ====\n",
    "def coerce_to_str(x) -> str:\n",
    "    if isinstance(x, list):\n",
    "        x = \" \".join(map(str, x))\n",
    "    elif pd.isna(x):\n",
    "        x = \"\"\n",
    "    return str(x).strip()\n",
    "\n",
    "def safe_augment(aug, text: str, n: int = 1) -> List[str]:\n",
    "    \"\"\"Retourne une liste de n paraphrases (peut être < n si le modèle échoue).\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    try:\n",
    "        out = aug.augment(text, n=n)  # peut renvoyer str ou list[str]\n",
    "        if isinstance(out, str):\n",
    "            out = [out]\n",
    "        return [t.strip() for t in out if isinstance(t, str) and t.strip()]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def nettoyer_texte_tokens(texte: str) -> List[str]:\n",
    "    tokens = word_tokenize(texte)\n",
    "    tokens_nettoyes = []\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        token = re.sub(r'\\s+', '', token)\n",
    "        token = re.sub(r'[^a-zàâçéèêëîïôûùüÿñæœ]', '', token)\n",
    "        if token and token not in stop_words and token not in punctuation and token not in custom_stopwords:\n",
    "            token = lemmatizer.lemmatize(token)\n",
    "            tokens_nettoyes.append(token)\n",
    "    return tokens_nettoyes\n",
    "\n",
    "def tokens_to_text(tokens: List[str]) -> str:\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# ==== Device & models ====\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device choisi :\", device)\n",
    "\n",
    "from_model, to_model = (\n",
    "    (\"facebook/wmt19-en-de\", \"facebook/wmt19-de-en\") if device == \"cuda\"\n",
    "    else (\"Helsinki-NLP/opus-mt-en-de\", \"Helsinki-NLP/opus-mt-de-en\")\n",
    ")\n",
    "\n",
    "back_trans_aug = naw.BackTranslationAug(\n",
    "    from_model_name=from_model,\n",
    "    to_model_name=to_model,\n",
    "    device=device,\n",
    "    batch_size=32 if device == \"cuda\" else 8,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# ==== Load data ====\n",
    "df = pd.read_csv(\"./data/data_final_phase2_private.csv\")\n",
    "df[\"text\"] = df[\"text\"].apply(coerce_to_str)\n",
    "\n",
    "# (Re)crée text_clean / token_clean si manquants\n",
    "if \"token_clean\" not in df.columns:\n",
    "    df[\"token_clean\"] = df[\"text\"].apply(nettoyer_texte_tokens)\n",
    "else:\n",
    "    mask = df[\"token_clean\"].isna()\n",
    "    df.loc[mask, \"token_clean\"] = df.loc[mask, \"text\"].apply(nettoyer_texte_tokens)\n",
    "\n",
    "if \"text_clean\" not in df.columns:\n",
    "    df[\"text_clean\"] = df[\"token_clean\"].apply(tokens_to_text)\n",
    "else:\n",
    "    mask = df[\"text_clean\"].isna()\n",
    "    df.loc[mask, \"text_clean\"] = df.loc[mask, \"token_clean\"].apply(tokens_to_text)\n",
    "\n",
    "# ==== Filtrer uniquement la classe VS ====\n",
    "\n",
    "mask_vs = df[\"type_article\"].astype(str).str.upper().eq(\"VS\")\n",
    "df_vs = df[mask_vs].copy()\n",
    "\n",
    "\n",
    "\n",
    "# --- Comptage ---\n",
    "count_vs  = (df[\"type_article\"].astype(str).str.upper() == \"VS\").sum()\n",
    "count_nvs = (df[\"type_article\"].astype(str).str.upper() == \"NVS\").sum()\n",
    "print(\"NVS:\", count_nvs, \" VS:\", count_vs)\n",
    "\n",
    "target = count_nvs\n",
    "needed = max(0, target - count_vs)\n",
    "if needed == 0:\n",
    "    print(\"Pas besoin d'augmentation: VS est déjà ≥ NVS.\")\n",
    "\n",
    "# --- Génération ---\n",
    "aug_rows = []\n",
    "if needed > 0:\n",
    "    factor = math.ceil(target / count_vs)  # nb total de versions par article (original compris)\n",
    "    print(f\"Chaque article VS doit produire à peu près {factor} versions (dont l'original).\")\n",
    "\n",
    "    for _, row in df_vs.iterrows():\n",
    "        raw = row[\"text\"]\n",
    "        aug_texts = safe_augment(back_trans_aug, raw[:2000], n=max(1, factor-1))\n",
    "        # construire les lignes\n",
    "        for aug_text in aug_texts:\n",
    "            tokens_bt = nettoyer_texte_tokens(aug_text)\n",
    "            aug_rows.append({\n",
    "                \"text_src\": raw,\n",
    "                \"text_final\": aug_text,\n",
    "                \"source\": \"bt\",\n",
    "                \"token_clean\": tokens_bt,\n",
    "                \"text_clean\": tokens_to_text(tokens_bt),\n",
    "                \"type_article\": row[\"type_article\"],\n",
    "                \"thematique\": row.get(\"thematique\", \"\")\n",
    "            })\n",
    "\n",
    "# DataFrame des augmentées\n",
    "aug_df = pd.DataFrame(aug_rows)\n",
    "\n",
    "# --- Déduplication robuste ---\n",
    "# 1) enlever les lignes vides/NaN\n",
    "aug_df = aug_df[aug_df[\"text_final\"].astype(str).str.strip().ne(\"\")].dropna(subset=[\"text_final\"])\n",
    "\n",
    "# 2) dédupliquer par paraphrase (et étiquette) pour éviter redites exactes\n",
    "aug_df = aug_df.drop_duplicates(subset=[\"text_final\", \"type_article\"], keep=\"first\")\n",
    "\n",
    "# 3) si trop de lignes, échantillonner pour viser exactement \"needed\"\n",
    "if len(aug_df) > needed:\n",
    "    # Utilisons random_sate pour controler l'aléa \n",
    "    aug_df = aug_df.sample(n=needed, random_state=42).reset_index(drop=True)\n",
    "elif len(aug_df) < needed:\n",
    "    print(f\"Seulement {len(aug_df)} paraphrases uniques générées, < needed={needed}.\")\n",
    "  \n",
    "\n",
    "print(\"Articles VS générés (uniques) :\", len(aug_df))\n",
    "print(\"Total VS (original + aug) :\", count_vs + len(aug_df))\n",
    "print(\"Total NVS :\", count_nvs)\n",
    "\n",
    "# --- Bloc original & concat finale ---\n",
    "train_original = df.copy()\n",
    "train_original[\"text_src\"]   = train_original[\"text\"]\n",
    "train_original[\"text_final\"] = train_original[\"text\"]\n",
    "train_original[\"source\"]     = \"orig\"\n",
    "\n",
    "cols = [\"text_final\", \"text_clean\", \"token_clean\", \"source\", \"type_article\", \"thematique\", \"text_src\"]\n",
    "\n",
    "def ensure_list(x):\n",
    "    if isinstance(x, list): return x\n",
    "    if isinstance(x, str):  return x.split()\n",
    "    return []\n",
    "\n",
    "train_original = train_original.reindex(columns=cols, fill_value=\"\")\n",
    "train_original[\"token_clean\"] = train_original[\"token_clean\"].apply(ensure_list)\n",
    "train_original[\"text_clean\"]  = train_original[\"text_clean\"].astype(str)\n",
    "\n",
    "train_aug = aug_df.reindex(columns=cols, fill_value=\"\")\n",
    "\n",
    "train_data = pd.concat([train_original, train_aug], ignore_index=True)\n",
    "print(\"Nb lignes original :\", len(df))\n",
    "print(\"Nb VS augmentées   :\", len(aug_df))\n",
    "print(\"Taille finale       :\", train_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2551be81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_final</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>token_clean</th>\n",
       "      <th>source</th>\n",
       "      <th>type_article</th>\n",
       "      <th>thematique</th>\n",
       "      <th>text_src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Microbial Community Composition Associated wit...</td>\n",
       "      <td>microbial community composition associated pot...</td>\n",
       "      <td>[microbial, community, composition, associated...</td>\n",
       "      <td>orig</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "      <td>Microbial Community Composition Associated wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Plant Pathogenic and Endophytic Colletotrichum...</td>\n",
       "      <td>plant pathogenic endophytic colletotrichum fru...</td>\n",
       "      <td>[plant, pathogenic, endophytic, colletotrichum...</td>\n",
       "      <td>orig</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "      <td>Plant Pathogenic and Endophytic Colletotrichum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lethal Bronzing: What you should know about th...</td>\n",
       "      <td>lethal bronzing know disease turn palm tree br...</td>\n",
       "      <td>[lethal, bronzing, know, disease, turn, palm, ...</td>\n",
       "      <td>orig</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "      <td>Lethal Bronzing: What you should know about th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leaffooted Bug Damage in Almond Orchards Leaff...</td>\n",
       "      <td>leaffooted bug damage almond orchard leaffoote...</td>\n",
       "      <td>[leaffooted, bug, damage, almond, orchard, lea...</td>\n",
       "      <td>orig</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "      <td>Leaffooted Bug Damage in Almond Orchards Leaff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kebbi govt battles mysterious disease affectin...</td>\n",
       "      <td>kebbi govt battle mysterious disease affecting...</td>\n",
       "      <td>[kebbi, govt, battle, mysterious, disease, aff...</td>\n",
       "      <td>orig</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "      <td>Kebbi govt battles mysterious disease affectin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2734</th>\n",
       "      <td>Mystery Seed Packages Appearing Once Again in ...</td>\n",
       "      <td>mystery seed package appearing alabama mystery...</td>\n",
       "      <td>[mystery, seed, package, appearing, alabama, m...</td>\n",
       "      <td>bt</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "      <td>Mystery Seed Packages Appearing Once Again in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2735</th>\n",
       "      <td>ACES: Mystery seed packages appeared again in ...</td>\n",
       "      <td>ace mystery seed package appeared alabama ace ...</td>\n",
       "      <td>[ace, mystery, seed, package, appeared, alabam...</td>\n",
       "      <td>bt</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "      <td>ACES: Mystery seed packages appearing once aga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736</th>\n",
       "      <td>Farmers blame the plague: 150,000 per bag Farm...</td>\n",
       "      <td>farmer blame plague per bag farmer blame plagu...</td>\n",
       "      <td>[farmer, blame, plague, per, bag, farmer, blam...</td>\n",
       "      <td>bt</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "      <td>Farmers Blame Unknown Pest As Pepper Hits ₦150...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2737</th>\n",
       "      <td>Sharp drop in yield due to mysterious fungal i...</td>\n",
       "      <td>sharp drop yield due mysterious fungal infecti...</td>\n",
       "      <td>[sharp, drop, yield, due, mysterious, fungal, ...</td>\n",
       "      <td>bt</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "      <td>Sharp decline in yield as mysterious fungal in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2738</th>\n",
       "      <td>According to Prairie Crop Disease Disease Moni...</td>\n",
       "      <td>according prairie crop disease disease monitor...</td>\n",
       "      <td>[according, prairie, crop, disease, disease, m...</td>\n",
       "      <td>bt</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "      <td>Physiological leaf spot suspected in southern ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2739 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_final  \\\n",
       "0     Microbial Community Composition Associated wit...   \n",
       "1     Plant Pathogenic and Endophytic Colletotrichum...   \n",
       "2     Lethal Bronzing: What you should know about th...   \n",
       "3     Leaffooted Bug Damage in Almond Orchards Leaff...   \n",
       "4     Kebbi govt battles mysterious disease affectin...   \n",
       "...                                                 ...   \n",
       "2734  Mystery Seed Packages Appearing Once Again in ...   \n",
       "2735  ACES: Mystery seed packages appeared again in ...   \n",
       "2736  Farmers blame the plague: 150,000 per bag Farm...   \n",
       "2737  Sharp drop in yield due to mysterious fungal i...   \n",
       "2738  According to Prairie Crop Disease Disease Moni...   \n",
       "\n",
       "                                             text_clean  \\\n",
       "0     microbial community composition associated pot...   \n",
       "1     plant pathogenic endophytic colletotrichum fru...   \n",
       "2     lethal bronzing know disease turn palm tree br...   \n",
       "3     leaffooted bug damage almond orchard leaffoote...   \n",
       "4     kebbi govt battle mysterious disease affecting...   \n",
       "...                                                 ...   \n",
       "2734  mystery seed package appearing alabama mystery...   \n",
       "2735  ace mystery seed package appeared alabama ace ...   \n",
       "2736  farmer blame plague per bag farmer blame plagu...   \n",
       "2737  sharp drop yield due mysterious fungal infecti...   \n",
       "2738  according prairie crop disease disease monitor...   \n",
       "\n",
       "                                            token_clean source type_article  \\\n",
       "0     [microbial, community, composition, associated...   orig           VS   \n",
       "1     [plant, pathogenic, endophytic, colletotrichum...   orig           VS   \n",
       "2     [lethal, bronzing, know, disease, turn, palm, ...   orig           VS   \n",
       "3     [leaffooted, bug, damage, almond, orchard, lea...   orig           VS   \n",
       "4     [kebbi, govt, battle, mysterious, disease, aff...   orig           VS   \n",
       "...                                                 ...    ...          ...   \n",
       "2734  [mystery, seed, package, appearing, alabama, m...     bt           VS   \n",
       "2735  [ace, mystery, seed, package, appeared, alabam...     bt           VS   \n",
       "2736  [farmer, blame, plague, per, bag, farmer, blam...     bt           VS   \n",
       "2737  [sharp, drop, yield, due, mysterious, fungal, ...     bt           VS   \n",
       "2738  [according, prairie, crop, disease, disease, m...     bt           VS   \n",
       "\n",
       "     thematique                                           text_src  \n",
       "0            SV  Microbial Community Composition Associated wit...  \n",
       "1            SV  Plant Pathogenic and Endophytic Colletotrichum...  \n",
       "2            SV  Lethal Bronzing: What you should know about th...  \n",
       "3            SV  Leaffooted Bug Damage in Almond Orchards Leaff...  \n",
       "4            SV  Kebbi govt battles mysterious disease affectin...  \n",
       "...         ...                                                ...  \n",
       "2734         SV  Mystery Seed Packages Appearing Once Again in ...  \n",
       "2735         SV  ACES: Mystery seed packages appearing once aga...  \n",
       "2736         SV  Farmers Blame Unknown Pest As Pepper Hits ₦150...  \n",
       "2737         SV  Sharp decline in yield as mysterious fungal in...  \n",
       "2738         SV  Physiological leaf spot suspected in southern ...  \n",
       "\n",
       "[2739 rows x 7 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a58f4b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"text_final\"].iloc[2735] == train_data[\"text_src\"].iloc[2735]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7966f22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type_article\n",
       "NVS    2241\n",
       "VS      498\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"type_article\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5dd94dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type_article\n",
       "NVS    2241\n",
       "VS      249\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"type_article\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5284e3e",
   "metadata": {},
   "source": [
    "#### Les textes augmentées par la rétro traduction ont une source bt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8a46c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sauvegardons les données augmentées dans un fichier csv \n",
    "\n",
    "train_data.to_csv(\"./data/data_augmented_back_traduction.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c982ca27",
   "metadata": {},
   "source": [
    "###  Nous allons reprendre la classification avec le fine-tuning de SBERT que nous avons fait à la phase 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4058528",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/data_augmented_back_traduction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "039b8800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['NVS', 'VS']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3715 is out of bounds for axis 0 with size 2739",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mClasses:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(label_encoder.classes_))\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Splits\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m X_train, X_val, X_test = \u001b[43mX_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_train\u001b[49m\u001b[43m]\u001b[49m, X_all[idx_val], X_all[idx_test]\n\u001b[32m     56\u001b[39m y_train, y_val, y_test = y_enc[idx_train], y_enc[idx_val], y_enc[idx_test]\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# 2) Tokenizer & modèle\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: index 3715 is out of bounds for axis 0 with size 2739"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 0) Imports\n",
    "# =========================\n",
    "import os, json, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    f1_score, precision_score, recall_score, accuracy_score\n",
    ")\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, TrainerCallback\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# GPU / précision (Ada -> bf16 recommandé)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "use_fp16 = torch.cuda.is_available() and not use_bf16\n",
    "\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Données + indices sauvegardés\n",
    "# =========================\n",
    "idx_train = idx_train_all\n",
    "idx_val   = idx_val_all\n",
    "idx_test  = idx_test_all\n",
    "\n",
    "X_all = data[\"text_final\"].astype(str).to_numpy() # Nous lançons l'entrainnement sur les données brutes \n",
    "y_all = data[\"type_article\"].to_numpy()\n",
    "\n",
    "# Encodage labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_enc = label_encoder.fit_transform(y_all)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", list(label_encoder.classes_))\n",
    "\n",
    "# Splits\n",
    "X_train, X_val, X_test = X_all[idx_train], X_all[idx_val], X_all[idx_test]\n",
    "y_train, y_val, y_test = y_enc[idx_train], y_enc[idx_val], y_enc[idx_test]\n",
    "\n",
    "# =========================\n",
    "# 2) Tokenizer & modèle\n",
    "# =========================\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_classes,\n",
    "    torch_dtype=(torch.bfloat16 if use_bf16 else None),  # bf16\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) Dataset PyTorch (robuste)\n",
    "# =========================\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        if isinstance(texts, np.ndarray):\n",
    "            texts = texts.tolist()\n",
    "        self.texts = [\n",
    "            \"\" if (t is None or (isinstance(t, float) and np.isnan(t))) else str(t)\n",
    "            for t in texts\n",
    "        ]\n",
    "        self.encodings = tokenizer(self.texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels.tolist() if isinstance(labels, np.ndarray) else list(labels)\n",
    "        assert len(self.texts) == len(self.labels)\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_ds = TextDataset(X_train, y_train, tokenizer)\n",
    "val_ds   = TextDataset(X_val,   y_val,   tokenizer)\n",
    "test_ds  = TextDataset(X_test,  y_test,  tokenizer)\n",
    "\n",
    "# =========================\n",
    "# 4) Metrics + Callback TrainEval\n",
    "# =========================\n",
    "def compute_metrics(eval_pred):\n",
    "    # Compat: EvalPrediction (HF récent) ou tuple (anciens)\n",
    "    if hasattr(eval_pred, \"predictions\"):\n",
    "        logits = eval_pred.predictions\n",
    "        labels = eval_pred.label_ids\n",
    "    else:\n",
    "        logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
    "        \"precision_macro\": precision_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"recall_macro\": recall_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "    }\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "import math, os\n",
    "\n",
    "class ValEvalAndEarlyStopCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    - Évalue sur le set de validation à la fin de chaque epoch\n",
    "    - Loggue les métriques 'eval_*' dans log_history\n",
    "    - Early stopping custom basé sur training_args.metric_for_best_model et greater_is_better\n",
    "    - Sauvegarde le meilleur modèle dans output_dir/best_checkpoint\n",
    "    \"\"\"\n",
    "    def __init__(self, trainer, patience=3):\n",
    "        self.trainer = trainer\n",
    "        self.patience = patience\n",
    "        self.best_metric = None\n",
    "        self.best_epoch = None\n",
    "        self.bad_epochs = 0\n",
    "        self.best_dir = os.path.join(str(trainer.args.output_dir), \"best_checkpoint\")\n",
    "        os.makedirs(self.best_dir, exist_ok=True)\n",
    "\n",
    "        # lit la config d'arrêt\n",
    "        self.metric_name = trainer.args.metric_for_best_model or \"eval_loss\"\n",
    "        self.greater_is_better = bool(trainer.args.greater_is_better)\n",
    "\n",
    "    def _is_better(self, current, best):\n",
    "        if best is None:\n",
    "            return True\n",
    "        return (current > best) if self.greater_is_better else (current < best)\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # 1) Évaluer VAL\n",
    "        try:\n",
    "            metrics = self.trainer.evaluate(\n",
    "                eval_dataset=self.trainer.eval_dataset,\n",
    "                metric_key_prefix=\"eval\"\n",
    "            )\n",
    "        except TypeError:\n",
    "            metrics = self.trainer.evaluate(eval_dataset=self.trainer.eval_dataset)\n",
    "\n",
    "        # 2) Ajouter l'epoch et logger explicitement\n",
    "        if state.epoch is not None:\n",
    "            metrics[\"epoch\"] = float(state.epoch)\n",
    "        self.trainer.log({k: float(v) for k, v in metrics.items()\n",
    "                          if isinstance(v, (int, float, np.floating))})\n",
    "\n",
    "        # 3) Early stopping custom\n",
    "        current = metrics.get(self.metric_name, None)\n",
    "        if current is None or (isinstance(current, float) and math.isnan(current)):\n",
    "            # rien à faire si la métrique n'est pas là\n",
    "            return control\n",
    "\n",
    "        if self._is_better(current, self.best_metric):\n",
    "            # Amélioration -> reset patience + save best\n",
    "            self.best_metric = current\n",
    "            self.best_epoch = int(round(state.epoch)) if state.epoch is not None else None\n",
    "            self.bad_epochs = 0\n",
    "            # Sauvegarder le meilleur modèle\n",
    "            self.trainer.save_model(self.best_dir)\n",
    "            if hasattr(self.trainer, \"tokenizer\") and self.trainer.tokenizer is not None:\n",
    "                self.trainer.tokenizer.save_pretrained(self.best_dir)\n",
    "            control.should_save = True\n",
    "        else:\n",
    "            # Pas d'amélioration\n",
    "            self.bad_epochs += 1\n",
    "            if self.bad_epochs >= self.patience:\n",
    "                # Stopper l'entraînement\n",
    "                control.should_training_stop = True\n",
    "\n",
    "        return control\n",
    "    \n",
    "\n",
    "# Pour avoir aussi les retours sur le jeux d'entrainement\n",
    "class TrainEvalCallback(TrainerCallback):\n",
    "    def __init__(self, trainer, train_dataset):\n",
    "        self.trainer = trainer\n",
    "        self.train_dataset = train_dataset\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Évalue sur TRAIN et logue train_*\n",
    "        try:\n",
    "            metrics = self.trainer.evaluate(\n",
    "                eval_dataset=self.train_dataset,\n",
    "                metric_key_prefix=\"train\"  # => train_loss, train_accuracy, ...\n",
    "            )\n",
    "        except TypeError:\n",
    "            metrics = self.trainer.evaluate(eval_dataset=self.train_dataset)\n",
    "            # Rétro-compat: préfixer manuellement si besoin\n",
    "            metrics = {f\"train_{k}\": v for k, v in metrics.items()}\n",
    "\n",
    "        if state.epoch is not None:\n",
    "            metrics[\"epoch\"] = float(state.epoch)\n",
    "\n",
    "        # pousse dans log_history (même avec report_to='none')\n",
    "        self.trainer.log({k: float(v) for k, v in metrics.items() if isinstance(v, (int, float, np.floating))})\n",
    "        return control\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) TrainingArguments + Trainer + EarlyStopping\n",
    "# =========================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_SBERT\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "\n",
    "    # on laisse les callbacks gérer eval/save/log par epoch\n",
    "    logging_dir=\"./logs_SBERT\",\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=3,\n",
    "    seed=SEED,\n",
    "\n",
    "    # GPU-friendly\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=4,       # qu'on peut ajuster\n",
    "    gradient_checkpointing=True,    # réduit la VRAM\n",
    "    bf16=use_bf16,                  # Ada -> True\n",
    "    fp16=use_fp16,                  # fallback si pas de bf16\n",
    "    gradient_accumulation_steps=1,  \n",
    "\n",
    "    # IMPORTANT: on ne dépend pas du best interne du Trainer\n",
    "    load_best_model_at_end=False,\n",
    "\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer, \n",
    "    callbacks=[]  # <= pas de EarlyStoppingCallback natif\n",
    ")\n",
    "\n",
    "# Callback VAL + early stop custom\n",
    "val_es_cb = ValEvalAndEarlyStopCallback(trainer, patience=3)\n",
    "trainer.add_callback(val_es_cb)\n",
    "\n",
    "# Callback TRAIN pour avoir train_* dans les courbes\n",
    "train_cb = TrainEvalCallback(trainer, train_ds)\n",
    "trainer.add_callback(train_cb)\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6) Entraînement\n",
    "# =========================\n",
    "print(f\"Device: {device} | CUDA={torch.cuda.is_available()} | bf16={use_bf16} | fp16={use_fp16}\")\n",
    "trainer.train()\n",
    "\n",
    "# =========================\n",
    "# 7) Historique pour tracés (robuste aux clés manquantes)\n",
    "# =========================\n",
    "log_hist = trainer.state.log_history\n",
    "hist_df = pd.DataFrame(log_hist)\n",
    "\n",
    "# Crée toutes les colonnes attendues si absentes pour éviter KeyError\n",
    "needed_cols = [\"epoch\",\"eval_loss\",\"eval_accuracy\",\"eval_f1_macro\",\n",
    "          \"train_loss\",\"train_accuracy\",\"train_f1_macro\",\n",
    "          \"train_precision_macro\",\"train_recall_macro\"]\n",
    "for c in needed_cols:\n",
    "    if c not in hist_df.columns:\n",
    "        hist_df[c] = np.nan\n",
    "\n",
    "# Ne garder que les lignes avec epoch défini et forcer epoch en int\n",
    "hist_df = hist_df[pd.to_numeric(hist_df[\"epoch\"], errors=\"coerce\").notna()].copy()\n",
    "hist_df[\"epoch\"] = hist_df[\"epoch\"].astype(float).round().astype(int)\n",
    "\n",
    "# Convertir toutes les métriques numériques en float (coerce -> NaN si non num)\n",
    "metric_cols = [c for c in needed_cols if c != \"epoch\"]\n",
    "for c in metric_cols:\n",
    "    hist_df[c] = pd.to_numeric(hist_df[c], errors=\"coerce\")\n",
    "\n",
    "# Fonction utilitaire: dernière valeur non-NaN dans un groupe\n",
    "def last_not_nan(s: pd.Series):\n",
    "    s = s.dropna()\n",
    "    return s.iloc[-1] if len(s) else np.nan\n",
    "\n",
    "# Agrégation par epoch: on prend la DERNIÈRE valeur non-NaN de chaque métrique\n",
    "curves = (\n",
    "    hist_df\n",
    "    .sort_values([\"epoch\"])  # important: on veut la \"dernière\" dans l'ordre\n",
    "    .groupby(\"epoch\", as_index=False)\n",
    "    .agg({col: last_not_nan for col in metric_cols})\n",
    "    .sort_values(\"epoch\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"\\nAperçu des courbes (par epoch):\\n\", curves.head(10))\n",
    "# ============================================================================\n",
    "\n",
    "# Exemple de tracés \"safe\"\n",
    "def _safe_plot(x, y, label):\n",
    "    s = pd.Series(y)\n",
    "    if s.notna().any():\n",
    "        plt.plot(x, y, marker=\"o\", label=label)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "_safe_plot(curves[\"epoch\"], curves[\"train_loss\"], \"Train Loss\")\n",
    "_safe_plot(curves[\"epoch\"], curves[\"eval_loss\"],  \"Val Loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Courbe d'apprentissage (Loss)\")\n",
    "plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "_safe_plot(curves[\"epoch\"], curves[\"train_accuracy\"], \"Train Accuracy\")\n",
    "_safe_plot(curves[\"epoch\"], curves[\"eval_accuracy\"],  \"Val Accuracy\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy — Train vs Val\")\n",
    "plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "_safe_plot(curves[\"epoch\"], curves[\"train_f1_macro\"], \"Train F1 (macro)\")\n",
    "_safe_plot(curves[\"epoch\"], curves[\"eval_f1_macro\"],  \"Val F1 (macro)\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"F1 (macro)\"); plt.title(\"F1 macro — Train vs Val\")\n",
    "plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "\n",
    "# =========================\n",
    "# 9) Évaluation finale sur TEST\n",
    "# =========================\n",
    "# Recharger le meilleur modèle pour la phase TEST\n",
    "best_dir = os.path.join(training_args.output_dir, \"best_checkpoint\")\n",
    "if os.path.isdir(best_dir):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(best_dir).to(model.device)\n",
    "    trainer.model = model\n",
    "\n",
    "preds_test = trainer.predict(test_ds)\n",
    "y_pred = np.argmax(preds_test.predictions, axis=1)\n",
    "y_scores = torch.softmax(torch.tensor(preds_test.predictions), dim=1).numpy()\n",
    "\n",
    "print(\"\\n=== Rapport de classification (TEST) ===\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Matrice de confusion — TEST\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Courbe PR pour \"VS\" si présent\n",
    "classes = list(label_encoder.classes_)\n",
    "if \"VS\" in classes:\n",
    "    vs_idx = classes.index(\"VS\")\n",
    "    y_true_bin = (y_test == vs_idx).astype(int)\n",
    "    y_prob_vs = y_scores[:, vs_idx]\n",
    "    precision, recall, _ = precision_recall_curve(y_true_bin, y_prob_vs)\n",
    "    auc_pr = average_precision_score(y_true_bin, y_prob_vs)\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(recall, precision, label=f\"AUC-PR = {auc_pr:.3f}\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision–Recall (classe VS) — TEST\")\n",
    "    plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(\"La classe 'VS' n'est pas présente dans les labels. PR-curve sautée.\")\n",
    "\n",
    "# =========================\n",
    "# 10) Sauvegardes\n",
    "# =========================\n",
    "save_dir = \"results_SBERT\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "trainer.model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "joblib.dump(label_encoder, os.path.join(save_dir, \"label_encoder.joblib\"))\n",
    "\n",
    "curves.to_csv(os.path.join(save_dir, \"learning_curves.csv\"), index=False)\n",
    "with open(os.path.join(save_dir, \"log_history.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(log_hist, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nBest model saved to: {best_dir} (metric {training_args.metric_for_best_model} \"\n",
    "      f\"with greater_is_better={training_args.greater_is_better})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
