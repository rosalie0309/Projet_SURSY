{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5cb84e8",
   "metadata": {},
   "source": [
    "### Nous allons ici faire quelques applications d'augmentation des donn√©es textuelles en suivant un tutoriel sur medium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73361097",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nlpaug\n",
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f4c2112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.\n",
      "  \"class\": algorithms.Blowfish,\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n",
      "W0825 09:27:33.543000 15484 torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4a1ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Un exemple de texte qu'on veut reformuler \n",
    "text = \"The quick brown fox jumps over a lazy dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8ec41e",
   "metadata": {},
   "source": [
    "#### Synonym Replacement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "831efad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym Text:  ['The flying john brown fox jumps all over a lazy dog']\n"
     ]
    }
   ],
   "source": [
    "syn_aug = naw.synonym.SynonymAug(aug_src=\"wordnet\")\n",
    "synonym_text = syn_aug.augment(text)\n",
    "print(\"Synonym Text: \", synonym_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4eb4bc",
   "metadata": {},
   "source": [
    "#### Random Substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a89133a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substituted Text:  ['The quick brown _ _ over _ lazy dog']\n"
     ]
    }
   ],
   "source": [
    "sub_aug = naw.random.RandomWordAug(action='substitute')\n",
    "substituted_text = sub_aug.augment(text)\n",
    "print(\"Substituted Text: \", substituted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c34e66",
   "metadata": {},
   "source": [
    "### Random Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad081b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deletion Text:  ['The quick brown jumps over lazy']\n"
     ]
    }
   ],
   "source": [
    "del_aug = naw.random.RandomWordAug(action='delete')\n",
    "deletion_text = del_aug.augment(text)\n",
    "print(\"Deletion Text: \", deletion_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f062258d",
   "metadata": {},
   "source": [
    "### Random Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb44cd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swap Text:  ['Quick the brown fox over jumps a dog lazy']\n"
     ]
    }
   ],
   "source": [
    "swap_aug = naw.random.RandomWordAug(action='swap')\n",
    "swap_text = swap_aug.augment(text)\n",
    "print(\"Swap Text: \", swap_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9decddfb",
   "metadata": {},
   "source": [
    "### Back Translation\n",
    "\n",
    "Translate original text to other language (like french) and convert back to english language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a95e1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stginrae.AFRIQUE-TP02\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc86b55111f4d6e9071c0843683c66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/825 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stginrae.AFRIQUE-TP02\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\stginrae.AFRIQUE-TP02\\.cache\\huggingface\\hub\\models--facebook--wmt19-en-de. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3f58ee501d4ec18b0acc14bc947bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c7e921f1dd4d849d84799217defe92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd239f6098bd41539f9f914df08cb0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/825 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stginrae.AFRIQUE-TP02\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\stginrae.AFRIQUE-TP02\\.cache\\huggingface\\hub\\models--facebook--wmt19-de-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c3dbfb06a245929e1d9909cebb38f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb25e101fed64c8d841a3db4dea89863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/260 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42790a89c3c34dc6b2bee22ceab70170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/67.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a47f7d3ecd4c0eb45711b57722f1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab-src.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960917324ea6403ab1a893e62968e912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab-tgt.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30d9275864f49c5a8518548f1b6d70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d09d2b38ec8479c80cac336ca9993e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/67.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6350d99001e84279bbfe9f34f529700e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab-src.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1099b9342a9d48d9b5f2acf312b1b013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab-tgt.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f96dfc6c03a40da9da36ae99dde539f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back Translated Text:  ['The speedy brown fox jumps over a lazy dog']\n"
     ]
    }
   ],
   "source": [
    "back_trans_aug = naw.back_translation.BackTranslationAug()\n",
    "back_trans_text = back_trans_aug.augment(text)\n",
    "print(\"Back Translated Text: \", back_trans_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff9bda2",
   "metadata": {},
   "source": [
    "### Nous allons appliquer la R√©trotraduction pour former notre premier jeu de donn√©es augment√© \n",
    "Nous allons appliquer cela sur les donn√©es de texte brute ensuite on fera encore le nettoyage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5199f43b",
   "metadata": {},
   "source": [
    "#### Fonction de nettoyage qu'on applique depuis √† nos textes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c19d7b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "\n",
    "# Initialisation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "custom_stopwords = {'et', 'al'} # Les expressions que nous avons retrouv√© en grande quantit√© lors de nos exp√©rimentations et nous avons d√©cid√© de ne pas les conserver \n",
    "\n",
    "\n",
    "def nettoyer_texte(texte):\n",
    "    \"\"\"\n",
    "    Nettoie un texte brut :\n",
    "    2. Tokenisation\n",
    "    3. Nettoyage caract√®res sp√©ciaux\n",
    "    4. Suppression stopwords, ponctuation, custom_stopwords\n",
    "    5. Lemmatisation\n",
    "    \"\"\"\n",
    "    \n",
    "    # √âtape 2 : tokenisation\n",
    "    tokens = word_tokenize(texte)\n",
    "\n",
    "    # √âtape 3-6 : nettoyage\n",
    "    tokens_nettoyes = []\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        token = re.sub(r'\\s+', '', token)\n",
    "        token = re.sub(r'[^a-z√†√¢√ß√©√®√™√´√Æ√Ø√¥√ª√π√º√ø√±√¶≈ì]', '', token)\n",
    "\n",
    "        if (\n",
    "            token\n",
    "            and token not in stop_words\n",
    "            and token not in punctuation\n",
    "            and token not in custom_stopwords\n",
    "        ):\n",
    "            token = lemmatizer.lemmatize(token)\n",
    "            tokens_nettoyes.append(token)\n",
    "\n",
    "    return ' '.join(tokens_nettoyes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3f76dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stginrae.AFRIQUE-TP02\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(coerce_to_str)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# --- 3) Back-translation (sur le texte brut, puis on re-nettoie) ---\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m back_trans_aug \u001b[38;5;241m=\u001b[39m naw\u001b[38;5;241m.\u001b[39mBackTranslationAug(\n\u001b[0;32m     44\u001b[0m     from_model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/wmt19-en-de\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     45\u001b[0m     to_model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/wmt19-de-en\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     46\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# ou \"cpu\" si pas de GPU\u001b[39;00m\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Tu peux choisir un sous-√©chantillon au d√©but pour tester\u001b[39;00m\n\u001b[0;32m     50\u001b[0m subset \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m200\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df)), random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nlpaug\\augmenter\\word\\back_translation.py:61\u001b[0m, in \u001b[0;36mBackTranslationAug.__init__\u001b[1;34m(self, from_model_name, to_model_name, name, device, batch_size, max_length, force_reload, verbose)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, from_model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacebook/wmt19-en-de\u001b[39m\u001b[38;5;124m'\u001b[39m, to_model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacebook/wmt19-de-en\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     56\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBackTranslationAug\u001b[39m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, force_reload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     58\u001b[0m         action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubstitute\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39mname, aug_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, aug_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, aug_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     59\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice, verbose\u001b[38;5;241m=\u001b[39mverbose, include_detail\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model(from_model_name\u001b[38;5;241m=\u001b[39mfrom_model_name, to_model_name\u001b[38;5;241m=\u001b[39mto_model_name, \n\u001b[0;32m     62\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, max_length\u001b[38;5;241m=\u001b[39mmax_length\n\u001b[0;32m     63\u001b[0m     )\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nlpaug\\augmenter\\word\\back_translation.py:76\u001b[0m, in \u001b[0;36mBackTranslationAug.get_model\u001b[1;34m(cls, from_model_name, to_model_name, device, force_reload, batch_size, max_length)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(\u001b[38;5;28mcls\u001b[39m, from_model_name, to_model_name, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, force_reload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     75\u001b[0m               batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m init_back_translation_model(from_model_name, to_model_name, device,\n\u001b[0;32m     77\u001b[0m         force_reload, batch_size, max_length)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nlpaug\\augmenter\\word\\back_translation.py:25\u001b[0m, in \u001b[0;36minit_back_translation_model\u001b[1;34m(from_model_name, to_model_name, device, force_reload, batch_size, max_length)\u001b[0m\n\u001b[0;32m     21\u001b[0m     BACK_TRANSLATION_MODELS[model_name]\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m=\u001b[39m max_length\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m BACK_TRANSLATION_MODELS[model_name]\n\u001b[1;32m---> 25\u001b[0m model \u001b[38;5;241m=\u001b[39m nml\u001b[38;5;241m.\u001b[39mMtTransformers(src_model_name\u001b[38;5;241m=\u001b[39mfrom_model_name, tgt_model_name\u001b[38;5;241m=\u001b[39mto_model_name, \n\u001b[0;32m     26\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, max_length\u001b[38;5;241m=\u001b[39mmax_length)\n\u001b[0;32m     28\u001b[0m BACK_TRANSLATION_MODELS[model_name] \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nlpaug\\model\\lang_models\\machine_translation_transformers.py:25\u001b[0m, in \u001b[0;36mMtTransformers.__init__\u001b[1;34m(self, src_model_name, tgt_model_name, device, silence, batch_size, max_length)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_model_name)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_model_name)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py:2576\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   2572\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2573\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2574\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2575\u001b[0m         )\n\u001b[1;32m-> 2576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 915\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 915\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 915\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[0;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1336\u001b[0m             device,\n\u001b[0;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1338\u001b[0m             non_blocking,\n\u001b[0;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1340\u001b[0m         )\n\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1342\u001b[0m         device,\n\u001b[0;32m   1343\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1344\u001b[0m         non_blocking,\n\u001b[0;32m   1345\u001b[0m     )\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\cuda\\__init__.py:363\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m     )\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    367\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "# --- 1) Utils s√ªrs ---\n",
    "def coerce_to_str(x) -> str:\n",
    "    \"\"\"Force en string: join si liste, sinon cast, et strip espaces.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        x = \" \".join(map(str, x))\n",
    "    elif pd.isna(x):\n",
    "        x = \"\"\n",
    "    return str(x).strip()\n",
    "\n",
    "def simple_clean(text: str) -> str:\n",
    "    \"\"\"Nettoyage simple; remplace par ton pipeline habituel si tu en as un.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # espaces multiples -> un espace\n",
    "    return text.strip()\n",
    "\n",
    "def safe_augment(aug, text: str) -> Optional[str]:\n",
    "    \"\"\"Back-translation s√ªre: garantie de renvoyer une str ou None si √©chec.\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    try:\n",
    "        out = aug.augment(text)  # peut renvoyer str ou list[str]\n",
    "        if isinstance(out, list):\n",
    "            # si n>1, on prend la premi√®re; sinon, c'√©tait d√©j√† une liste\n",
    "            out = out[0] if out else None\n",
    "        return out.strip() if out else None\n",
    "    except Exception as e:\n",
    "        # log optionnel\n",
    "        # print(f\"[AUG-ERR] {e} | text head: {text[:120]}\")\n",
    "        return None\n",
    "\n",
    "# --- 2) Dataset ---\n",
    "df = pd.read_csv(\"./data/data_final_phase2_private.csv\")  # colonnes: \"text\", \"text_clean\",\n",
    "\n",
    "# Normaliser la colonne text (important pour tes exemples qui sont des listes/paragraphes)\n",
    "df[\"text\"] = df[\"text\"].apply(coerce_to_str)\n",
    "\n",
    "# --- 3) Back-translation (sur le texte brut, puis on re-nettoie) ---\n",
    "back_trans_aug = naw.BackTranslationAug(\n",
    "    from_model_name=\"facebook/wmt19-en-de\",\n",
    "    to_model_name=\"facebook/wmt19-de-en\",\n",
    "    device=\"cuda\"  # ou \"cpu\" si pas de GPU\n",
    ")\n",
    "\n",
    "# Tu peux choisir un sous-√©chantillon au d√©but pour tester\n",
    "subset = df.sample(min(200, len(df)), random_state=42).copy()\n",
    "\n",
    "aug_rows = []\n",
    "for idx, row in subset.iterrows():\n",
    "    raw = row[\"text\"]\n",
    "    # (optionnel) tronquer si tr√®s long pour √©viter OOM / timeouts\n",
    "    # raw = raw[:2000]\n",
    "\n",
    "    aug_text = safe_augment(back_trans_aug, raw)\n",
    "    if not aug_text:\n",
    "        continue  # skip proprement\n",
    "\n",
    "    aug_rows.append({\n",
    "        \"text\": raw,                  # original\n",
    "        \"text_aug\": aug_text,         # version back-translate\n",
    "        \"text_clean\": simple_clean(aug_text),\n",
    "        \"type_article\": row[\"type_article\"]\n",
    "    })\n",
    "\n",
    "aug_df = pd.DataFrame(aug_rows)\n",
    "\n",
    "# --- 4) Fusion (option 1 : concat √† part) ---\n",
    "# On garde df tel quel, et on a un df augment√© √† part :\n",
    "# aug_df: colonnes -> text (original), text_aug (nouveau), text_clean (nettoy√©), label (si pr√©sent)\n",
    "\n",
    "# --- 5) Fusion (option 2 : empiler comme nouvelles lignes pour entra√Æner) ---\n",
    "# Si tu veux entra√Æner sur une seule colonne \"text\" (m√©lange original + aug),\n",
    "# on duplique la structure et on renomme :\n",
    "train_original = df.rename(columns={\"text\": \"text_src\"}).copy()\n",
    "train_original[\"text_final\"] = train_original[\"text\"]\n",
    "train_original[\"source\"] = \"orig\"\n",
    "\n",
    "train_aug = aug_df.rename(columns={\"text_aug\": \"text_final\"}).copy()\n",
    "train_aug[\"text_src\"] = train_aug[\"text\"]\n",
    "train_aug[\"source\"] = \"bt\"\n",
    "\n",
    "# Aligner les colonnes utiles\n",
    "cols = [\"text_final\", \"text_clean\", \"source\"] + [\"type_article\"]\n",
    "train_data = pd.concat(\n",
    "    [\n",
    "        train_original.reindex(columns=cols, fill_value=\"\"),\n",
    "        train_aug.reindex(columns=cols, fill_value=\"\")\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Maintenant, tu peux vectoriser `train_data[\"text_clean\"]` (ou `text_final` + refaire ton pipeline de nettoyage complet).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
