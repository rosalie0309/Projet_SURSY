{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5cb84e8",
   "metadata": {},
   "source": [
    "### Nous allons ici faire quelques applications d'augmentation des données textuelles en suivant un tutoriel sur medium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73361097",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nlpaug\n",
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4c2112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rosalie/miniforge3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a1ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Un exemple de texte qu'on veut reformuler \n",
    "text = \"The quick brown fox jumps over a lazy dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b61473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nltk\n",
    "\n",
    "# 1) Dossier local et inscriptible pour les données NLTK\n",
    "NLTK_DIR = os.path.expanduser(\"~/nltk_data\")\n",
    "os.makedirs(NLTK_DIR, exist_ok=True)\n",
    "if NLTK_DIR not in nltk.data.path:\n",
    "    nltk.data.path.append(NLTK_DIR)\n",
    "\n",
    "# 2) Téléchargements nécessaires\n",
    "# - wordnet + omw-1.4 : pour les synonymes\n",
    "# - averaged_perceptron_tagger_eng : POS tagger (NLTK 3.8+)\n",
    "# - averaged_perceptron_tagger : par compatibilité descendante (certaines libs le demandent encore)\n",
    "# - punkt : tokenisation de base (utile selon les tokenizers)\n",
    "for pkg in [\"wordnet\", \"omw-1.4\", \"averaged_perceptron_tagger_eng\",\n",
    "            \"averaged_perceptron_tagger\", \"punkt\"]:\n",
    "    try:\n",
    "        nltk.download(pkg, download_dir=NLTK_DIR, quiet=True)\n",
    "    except Exception as e:\n",
    "        print(f\"NLTK download failed for {pkg}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8ec41e",
   "metadata": {},
   "source": [
    "#### Synonym Replacement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "831efad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym Text:  ['The quick robert brown fox bound over a lazy wienerwurst']\n"
     ]
    }
   ],
   "source": [
    "syn_aug = naw.synonym.SynonymAug(aug_src=\"wordnet\")\n",
    "synonym_text = syn_aug.augment(text)\n",
    "print(\"Synonym Text: \", synonym_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4eb4bc",
   "metadata": {},
   "source": [
    "#### Random Substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a89133a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substituted Text:  ['_ quick brown _ jumps over a lazy _']\n"
     ]
    }
   ],
   "source": [
    "sub_aug = naw.random.RandomWordAug(action='substitute')\n",
    "substituted_text = sub_aug.augment(text)\n",
    "print(\"Substituted Text: \", substituted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c34e66",
   "metadata": {},
   "source": [
    "### Random Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad081b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deletion Text:  ['The jumps over a lazy dog']\n"
     ]
    }
   ],
   "source": [
    "del_aug = naw.random.RandomWordAug(action='delete')\n",
    "deletion_text = del_aug.augment(text)\n",
    "print(\"Deletion Text: \", deletion_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f062258d",
   "metadata": {},
   "source": [
    "### Random Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb44cd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swap Text:  ['Quick the brown jumps fox over lazy a dog']\n"
     ]
    }
   ],
   "source": [
    "swap_aug = naw.random.RandomWordAug(action='swap')\n",
    "swap_text = swap_aug.augment(text)\n",
    "print(\"Swap Text: \", swap_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9decddfb",
   "metadata": {},
   "source": [
    "### Back Translation\n",
    "\n",
    "Translate original text to other language (like french) and convert back to english language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a95e1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back Translated Text:  ['The speedy brown fox jumps over a lazy dog']\n"
     ]
    }
   ],
   "source": [
    "back_trans_aug = naw.back_translation.BackTranslationAug()\n",
    "back_trans_text = back_trans_aug.augment(text)\n",
    "print(\"Back Translated Text: \", back_trans_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff9bda2",
   "metadata": {},
   "source": [
    "### Nous allons appliquer la Rétrotraduction pour former notre premier jeu de données augmenté \n",
    "Nous allons appliquer cela sur les données de texte brute ensuite on fera encore le nettoyage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b75e257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nltk\n",
    "\n",
    "# Dossier local pour les données NLTK (avec droits d’écriture)\n",
    "NLTK_DIR = os.path.expanduser(\"~/nltk_data\")\n",
    "os.makedirs(NLTK_DIR, exist_ok=True)\n",
    "if NLTK_DIR not in nltk.data.path:\n",
    "    nltk.data.path.append(NLTK_DIR)\n",
    "\n",
    "# Paquets requis (NLTK 3.8+)\n",
    "for pkg in [\n",
    "    \"punkt\",                         # tokeniseur historique\n",
    "    \"punkt_tab\",                     # depuis NLTK 3.8\n",
    "    \"stopwords\",\n",
    "    \"wordnet\", \"omw-1.4\",\n",
    "    \"averaged_perceptron_tagger_eng\",\n",
    "    \"averaged_perceptron_tagger\",    # compat descente\n",
    "]:\n",
    "    try:\n",
    "        nltk.download(pkg, download_dir=NLTK_DIR, quiet=True)\n",
    "    except Exception as e:\n",
    "        print(f\"NLTK download failed for {pkg}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f76dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device choisi : cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "from typing import Optional, List\n",
    "import torch\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "\n",
    "# ==== Init NLTK stuff ====\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "custom_stopwords = {'et', 'al'}\n",
    "\n",
    "# ==== Utils ====\n",
    "def coerce_to_str(x) -> str:\n",
    "    if isinstance(x, list):\n",
    "        x = \" \".join(map(str, x))\n",
    "    elif pd.isna(x):\n",
    "        x = \"\"\n",
    "    return str(x).strip()\n",
    "\n",
    "def safe_augment(aug, text: str) -> Optional[str]:\n",
    "    if not text:\n",
    "        return None\n",
    "    try:\n",
    "        out = aug.augment(text)\n",
    "        if isinstance(out, list):\n",
    "            out = out[0] if out else None\n",
    "        return out.strip() if out else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "#  retourne une LISTE de tokens nettoyés\n",
    "def nettoyer_texte_tokens(texte: str) -> List[str]:\n",
    "    tokens = word_tokenize(texte)\n",
    "    tokens_nettoyes = []\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        token = re.sub(r'\\s+', '', token)\n",
    "        token = re.sub(r'[^a-zàâçéèêëîïôûùüÿñæœ]', '', token)\n",
    "        if token and token not in stop_words and token not in punctuation and token not in custom_stopwords:\n",
    "            token = lemmatizer.lemmatize(token)\n",
    "            tokens_nettoyes.append(token)\n",
    "    return tokens_nettoyes\n",
    "\n",
    "def tokens_to_text(tokens: List[str]) -> str:\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# ==== Device & models ====\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device choisi :\", device)\n",
    "\n",
    "from_model, to_model = (\n",
    "    (\"facebook/wmt19-en-de\", \"facebook/wmt19-de-en\") if device == \"cuda\"\n",
    "    else (\"Helsinki-NLP/opus-mt-en-de\", \"Helsinki-NLP/opus-mt-de-en\")\n",
    ")\n",
    "\n",
    "back_trans_aug = naw.BackTranslationAug(\n",
    "    from_model_name=from_model,\n",
    "    to_model_name=to_model,\n",
    "    device=device,\n",
    "    batch_size=32 if device == \"cuda\" else 8,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# ==== Load data ====\n",
    "# attendu: colonnes min: text, type_article, thematique\n",
    "# optionnel: text_clean, token_clean\n",
    "df = pd.read_csv(\"./data/data_final_phase2_private.csv\")\n",
    "df[\"text\"] = df[\"text\"].apply(coerce_to_str)\n",
    "\n",
    "# Si token_clean/text_clean manquent ou sont NaN sur certaines lignes, on (re)calcule\n",
    "if \"token_clean\" not in df.columns:\n",
    "    df[\"token_clean\"] = df[\"text\"].apply(nettoyer_texte_tokens)\n",
    "else:\n",
    "    # Remplir les trous éventuels\n",
    "    df.loc[df[\"token_clean\"].isna(), \"token_clean\"] = df.loc[df[\"token_clean\"].isna(), \"text\"].apply(nettoyer_texte_tokens)\n",
    "\n",
    "if \"text_clean\" not in df.columns:\n",
    "    df[\"text_clean\"] = df[\"token_clean\"].apply(tokens_to_text)\n",
    "else:\n",
    "    # Si certaines lignes sont NaN, on les régénère\n",
    "    mask_nan = df[\"text_clean\"].isna()\n",
    "    df.loc[mask_nan, \"text_clean\"] = df.loc[mask_nan, \"token_clean\"].apply(tokens_to_text)\n",
    "\n",
    "# ==== Back-translation sur TOUT le dataset (1:1) ====\n",
    "subset = df  # ou df.sample(n=..., random_state=42) si tu nous voulons limiter\n",
    "\n",
    "aug_rows = []\n",
    "for _, row in subset.iterrows():\n",
    "    raw = row[\"text\"]\n",
    "    aug_text = safe_augment(back_trans_aug, raw[:2000])  # tronque si très long\n",
    "    if not aug_text:\n",
    "        continue\n",
    "\n",
    "    tokens_bt = nettoyer_texte_tokens(aug_text)\n",
    "    aug_rows.append({\n",
    "        # trace\n",
    "        \"text_src\": raw,              # texte source (original)\n",
    "        \"text_final\": aug_text,       # texte utilisé pour l'entraînement (version BT)\n",
    "        \"source\": \"bt\",\n",
    "\n",
    "        # colonnes \"clean\"\n",
    "        \"token_clean\": tokens_bt,     # liste de tokens\n",
    "        \"text_clean\": tokens_to_text(tokens_bt),\n",
    "\n",
    "        # labels/infos\n",
    "        \"type_article\": row[\"type_article\"],\n",
    "        \"thematique\": row.get(\"thematique\", \"\"),\n",
    "    })\n",
    "\n",
    "aug_df = pd.DataFrame(aug_rows)\n",
    "\n",
    "# ==== Prépare bloc original pour empilement ====\n",
    "train_original = df.copy()\n",
    "train_original[\"text_src\"] = train_original[\"text\"]          # garder la source\n",
    "train_original[\"text_final\"] = train_original[\"text\"]        # pour l'entraînement texte original\n",
    "train_original[\"source\"] = \"orig\"\n",
    "\n",
    "# ==== Harmoniser les colonnes ====\n",
    "cols = [\"text_final\", \"text_clean\", \"token_clean\", \"source\", \"type_article\", \"thematique\", \"text_src\"]\n",
    "\n",
    "# Attention: si dans le CSV d'origine `token_clean` est une chaîne (ex: \"word1 word2\"),\n",
    "# on préfère le convertir en liste pour cohérence. Sinon, laisse tel quel.\n",
    "# Ici on essaie de garantir une liste; sinon on la reconstruit depuis text_clean.\n",
    "def ensure_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        return x.split()\n",
    "    return []\n",
    "\n",
    "train_original = train_original.reindex(columns=cols, fill_value=\"\")\n",
    "train_original[\"token_clean\"] = train_original[\"token_clean\"].apply(ensure_list)\n",
    "train_original[\"text_clean\"] = train_original[\"text_clean\"].astype(str)\n",
    "\n",
    "train_aug = aug_df.reindex(columns=cols, fill_value=\"\")\n",
    "\n",
    "# ==== Empilement final ====\n",
    "train_data = pd.concat([train_original, train_aug], ignore_index=True)\n",
    "print(\"Taille finale du dataset :\", train_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2551be81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_final</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>source</th>\n",
       "      <th>type_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Microbial Community Composition Associated wit...</td>\n",
       "      <td>microbial community composition associated pot...</td>\n",
       "      <td>orig</td>\n",
       "      <td>VS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Plant Pathogenic and Endophytic Colletotrichum...</td>\n",
       "      <td>plant pathogenic endophytic colletotrichum fru...</td>\n",
       "      <td>orig</td>\n",
       "      <td>VS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lethal Bronzing: What you should know about th...</td>\n",
       "      <td>lethal bronzing know disease turn palm tree br...</td>\n",
       "      <td>orig</td>\n",
       "      <td>VS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leaffooted Bug Damage in Almond Orchards Leaff...</td>\n",
       "      <td>leaffooted bug damage almond orchard leaffoote...</td>\n",
       "      <td>orig</td>\n",
       "      <td>VS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kebbi govt battles mysterious disease affectin...</td>\n",
       "      <td>kebbi govt battle mysterious disease affecting...</td>\n",
       "      <td>orig</td>\n",
       "      <td>VS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2685</th>\n",
       "      <td>SHANGHAI (Reuters) - China confirmed outbreaks...</td>\n",
       "      <td>SHANGHAI (Reuters) - China confirmed outbreaks...</td>\n",
       "      <td>bt</td>\n",
       "      <td>NVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2686</th>\n",
       "      <td>The drama of the secular plants of -----------...</td>\n",
       "      <td>The drama of the secular plants of -----------...</td>\n",
       "      <td>bt</td>\n",
       "      <td>NVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>Postponement letter answers parliamentary ques...</td>\n",
       "      <td>Postponement letter answers parliamentary ques...</td>\n",
       "      <td>bt</td>\n",
       "      <td>NVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2688</th>\n",
       "      <td>Brazil: Fundecitrus annual study shows increas...</td>\n",
       "      <td>Brazil: Fundecitrus annual study shows increas...</td>\n",
       "      <td>bt</td>\n",
       "      <td>NVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2689</th>\n",
       "      <td>The outbreak of bird flu threatens the poultry...</td>\n",
       "      <td>The outbreak of bird flu threatens the poultry...</td>\n",
       "      <td>bt</td>\n",
       "      <td>NVS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2690 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_final  \\\n",
       "0     Microbial Community Composition Associated wit...   \n",
       "1     Plant Pathogenic and Endophytic Colletotrichum...   \n",
       "2     Lethal Bronzing: What you should know about th...   \n",
       "3     Leaffooted Bug Damage in Almond Orchards Leaff...   \n",
       "4     Kebbi govt battles mysterious disease affectin...   \n",
       "...                                                 ...   \n",
       "2685  SHANGHAI (Reuters) - China confirmed outbreaks...   \n",
       "2686  The drama of the secular plants of -----------...   \n",
       "2687  Postponement letter answers parliamentary ques...   \n",
       "2688  Brazil: Fundecitrus annual study shows increas...   \n",
       "2689  The outbreak of bird flu threatens the poultry...   \n",
       "\n",
       "                                             text_clean source type_article  \n",
       "0     microbial community composition associated pot...   orig           VS  \n",
       "1     plant pathogenic endophytic colletotrichum fru...   orig           VS  \n",
       "2     lethal bronzing know disease turn palm tree br...   orig           VS  \n",
       "3     leaffooted bug damage almond orchard leaffoote...   orig           VS  \n",
       "4     kebbi govt battle mysterious disease affecting...   orig           VS  \n",
       "...                                                 ...    ...          ...  \n",
       "2685  SHANGHAI (Reuters) - China confirmed outbreaks...     bt          NVS  \n",
       "2686  The drama of the secular plants of -----------...     bt          NVS  \n",
       "2687  Postponement letter answers parliamentary ques...     bt          NVS  \n",
       "2688  Brazil: Fundecitrus annual study shows increas...     bt          NVS  \n",
       "2689  The outbreak of bird flu threatens the poultry...     bt          NVS  \n",
       "\n",
       "[2690 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a58f4b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "orig    2490\n",
       "bt       200\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"source\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5284e3e",
   "metadata": {},
   "source": [
    "#### Les textes augmentées par la rétro traduction ont une source bt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8a46c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens_clean</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>type_article</th>\n",
       "      <th>thematique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Microbial Community Composition Associated wit...</td>\n",
       "      <td>['microbial', 'community', 'composition', 'ass...</td>\n",
       "      <td>microbial community composition associated pot...</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Plant Pathogenic and Endophytic Colletotrichum...</td>\n",
       "      <td>['plant', 'pathogenic', 'endophytic', 'colleto...</td>\n",
       "      <td>plant pathogenic endophytic colletotrichum fru...</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lethal Bronzing: What you should know about th...</td>\n",
       "      <td>['lethal', 'bronzing', 'know', 'disease', 'tur...</td>\n",
       "      <td>lethal bronzing know disease turn palm tree br...</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leaffooted Bug Damage in Almond Orchards Leaff...</td>\n",
       "      <td>['leaffooted', 'bug', 'damage', 'almond', 'orc...</td>\n",
       "      <td>leaffooted bug damage almond orchard leaffoote...</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kebbi govt battles mysterious disease affectin...</td>\n",
       "      <td>['kebbi', 'govt', 'battle', 'mysterious', 'dis...</td>\n",
       "      <td>kebbi govt battle mysterious disease affecting...</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>Ministry Asserts 59 Specimens Test Negative fo...</td>\n",
       "      <td>['ministry', 'asserts', 'specimen', 'test', 'n...</td>\n",
       "      <td>ministry asserts specimen test negative minist...</td>\n",
       "      <td>NVS</td>\n",
       "      <td>SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>China Ramps Up Imports of US Pork as  Spreads ...</td>\n",
       "      <td>['china', 'ramp', 'import', 'u', 'pork', 'spre...</td>\n",
       "      <td>china ramp import u pork spread china ramp imp...</td>\n",
       "      <td>NVS</td>\n",
       "      <td>SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487</th>\n",
       "      <td>ASF China: Large farm in Jiangsu hit; virus re...</td>\n",
       "      <td>['asf', 'china', 'large', 'farm', 'jiangsu', '...</td>\n",
       "      <td>asf china large farm jiangsu hit virus reach g...</td>\n",
       "      <td>NVS</td>\n",
       "      <td>SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2488</th>\n",
       "      <td>The CSIC leads an international project to mit...</td>\n",
       "      <td>['csic', 'lead', 'international', 'project', '...</td>\n",
       "      <td>csic lead international project mitigate damag...</td>\n",
       "      <td>NVS</td>\n",
       "      <td>SV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2489</th>\n",
       "      <td>Italians fear for their olive oil Italians fea...</td>\n",
       "      <td>['italian', 'fear', 'olive', 'oil', 'italian',...</td>\n",
       "      <td>italian fear olive oil italian fear olive oil ...</td>\n",
       "      <td>NVS</td>\n",
       "      <td>SV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2490 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Microbial Community Composition Associated wit...   \n",
       "1     Plant Pathogenic and Endophytic Colletotrichum...   \n",
       "2     Lethal Bronzing: What you should know about th...   \n",
       "3     Leaffooted Bug Damage in Almond Orchards Leaff...   \n",
       "4     Kebbi govt battles mysterious disease affectin...   \n",
       "...                                                 ...   \n",
       "2485  Ministry Asserts 59 Specimens Test Negative fo...   \n",
       "2486  China Ramps Up Imports of US Pork as  Spreads ...   \n",
       "2487  ASF China: Large farm in Jiangsu hit; virus re...   \n",
       "2488  The CSIC leads an international project to mit...   \n",
       "2489  Italians fear for their olive oil Italians fea...   \n",
       "\n",
       "                                           tokens_clean  \\\n",
       "0     ['microbial', 'community', 'composition', 'ass...   \n",
       "1     ['plant', 'pathogenic', 'endophytic', 'colleto...   \n",
       "2     ['lethal', 'bronzing', 'know', 'disease', 'tur...   \n",
       "3     ['leaffooted', 'bug', 'damage', 'almond', 'orc...   \n",
       "4     ['kebbi', 'govt', 'battle', 'mysterious', 'dis...   \n",
       "...                                                 ...   \n",
       "2485  ['ministry', 'asserts', 'specimen', 'test', 'n...   \n",
       "2486  ['china', 'ramp', 'import', 'u', 'pork', 'spre...   \n",
       "2487  ['asf', 'china', 'large', 'farm', 'jiangsu', '...   \n",
       "2488  ['csic', 'lead', 'international', 'project', '...   \n",
       "2489  ['italian', 'fear', 'olive', 'oil', 'italian',...   \n",
       "\n",
       "                                             text_clean type_article  \\\n",
       "0     microbial community composition associated pot...           VS   \n",
       "1     plant pathogenic endophytic colletotrichum fru...           VS   \n",
       "2     lethal bronzing know disease turn palm tree br...           VS   \n",
       "3     leaffooted bug damage almond orchard leaffoote...           VS   \n",
       "4     kebbi govt battle mysterious disease affecting...           VS   \n",
       "...                                                 ...          ...   \n",
       "2485  ministry asserts specimen test negative minist...          NVS   \n",
       "2486  china ramp import u pork spread china ramp imp...          NVS   \n",
       "2487  asf china large farm jiangsu hit virus reach g...          NVS   \n",
       "2488  csic lead international project mitigate damag...          NVS   \n",
       "2489  italian fear olive oil italian fear olive oil ...          NVS   \n",
       "\n",
       "     thematique  \n",
       "0            SV  \n",
       "1            SV  \n",
       "2            SV  \n",
       "3            SV  \n",
       "4            SV  \n",
       "...         ...  \n",
       "2485         SA  \n",
       "2486         SA  \n",
       "2487         SA  \n",
       "2488         SV  \n",
       "2489         SV  \n",
       "\n",
       "[2490 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
