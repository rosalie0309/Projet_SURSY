{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5cb84e8",
   "metadata": {},
   "source": [
    "### Nous allons ici faire quelques applications d'augmentation des données textuelles en suivant un tutoriel sur medium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73361097",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nlpaug\n",
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4c2112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rosalie/miniforge3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a1ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Un exemple de texte qu'on veut reformuler \n",
    "text = \"The quick brown fox jumps over a lazy dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b61473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nltk\n",
    "\n",
    "# 1) Dossier local et inscriptible pour les données NLTK\n",
    "NLTK_DIR = os.path.expanduser(\"~/nltk_data\")\n",
    "os.makedirs(NLTK_DIR, exist_ok=True)\n",
    "if NLTK_DIR not in nltk.data.path:\n",
    "    nltk.data.path.append(NLTK_DIR)\n",
    "\n",
    "# 2) Téléchargements nécessaires\n",
    "# - wordnet + omw-1.4 : pour les synonymes\n",
    "# - averaged_perceptron_tagger_eng : POS tagger (NLTK 3.8+)\n",
    "# - averaged_perceptron_tagger : par compatibilité descendante (certaines libs le demandent encore)\n",
    "# - punkt : tokenisation de base (utile selon les tokenizers)\n",
    "for pkg in [\"wordnet\", \"omw-1.4\", \"averaged_perceptron_tagger_eng\",\n",
    "            \"averaged_perceptron_tagger\", \"punkt\"]:\n",
    "    try:\n",
    "        nltk.download(pkg, download_dir=NLTK_DIR, quiet=True)\n",
    "    except Exception as e:\n",
    "        print(f\"NLTK download failed for {pkg}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8ec41e",
   "metadata": {},
   "source": [
    "#### Synonym Replacement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "831efad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym Text:  ['The quick robert brown fox bound over a lazy wienerwurst']\n"
     ]
    }
   ],
   "source": [
    "syn_aug = naw.synonym.SynonymAug(aug_src=\"wordnet\")\n",
    "synonym_text = syn_aug.augment(text)\n",
    "print(\"Synonym Text: \", synonym_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4eb4bc",
   "metadata": {},
   "source": [
    "#### Random Substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a89133a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substituted Text:  ['_ quick brown _ jumps over a lazy _']\n"
     ]
    }
   ],
   "source": [
    "sub_aug = naw.random.RandomWordAug(action='substitute')\n",
    "substituted_text = sub_aug.augment(text)\n",
    "print(\"Substituted Text: \", substituted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c34e66",
   "metadata": {},
   "source": [
    "### Random Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad081b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deletion Text:  ['The jumps over a lazy dog']\n"
     ]
    }
   ],
   "source": [
    "del_aug = naw.random.RandomWordAug(action='delete')\n",
    "deletion_text = del_aug.augment(text)\n",
    "print(\"Deletion Text: \", deletion_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f062258d",
   "metadata": {},
   "source": [
    "### Random Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb44cd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swap Text:  ['Quick the brown jumps fox over lazy a dog']\n"
     ]
    }
   ],
   "source": [
    "swap_aug = naw.random.RandomWordAug(action='swap')\n",
    "swap_text = swap_aug.augment(text)\n",
    "print(\"Swap Text: \", swap_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9decddfb",
   "metadata": {},
   "source": [
    "### Back Translation\n",
    "\n",
    "Translate original text to other language (like french) and convert back to english language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a95e1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back Translated Text:  ['The speedy brown fox jumps over a lazy dog']\n"
     ]
    }
   ],
   "source": [
    "back_trans_aug = naw.back_translation.BackTranslationAug()\n",
    "back_trans_text = back_trans_aug.augment(text)\n",
    "print(\"Back Translated Text: \", back_trans_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff9bda2",
   "metadata": {},
   "source": [
    "### Nous allons appliquer la Rétrotraduction pour former notre premier jeu de données augmenté \n",
    "Nous allons appliquer cela sur les données de texte brute ensuite on fera encore le nettoyage, nous allons appliquer l'augmentation uniquement pour les articles de type VS qui est sous représenté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b75e257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nltk\n",
    "\n",
    "# Dossier local pour les données NLTK (avec droits d’écriture)\n",
    "NLTK_DIR = os.path.expanduser(\"~/nltk_data\")\n",
    "os.makedirs(NLTK_DIR, exist_ok=True)\n",
    "if NLTK_DIR not in nltk.data.path:\n",
    "    nltk.data.path.append(NLTK_DIR)\n",
    "\n",
    "# Paquets requis (NLTK 3.8+)\n",
    "for pkg in [\n",
    "    \"punkt\",                         # tokeniseur historique\n",
    "    \"punkt_tab\",                     # depuis NLTK 3.8\n",
    "    \"stopwords\",\n",
    "    \"wordnet\", \"omw-1.4\",\n",
    "    \"averaged_perceptron_tagger_eng\",\n",
    "    \"averaged_perceptron_tagger\",    # compat descente\n",
    "]:\n",
    "    try:\n",
    "        nltk.download(pkg, download_dir=NLTK_DIR, quiet=True)\n",
    "    except Exception as e:\n",
    "        print(f\"NLTK download failed for {pkg}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "from typing import Optional, List\n",
    "import torch, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# ==== Init NLTK stuff ====\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "custom_stopwords = {'et', 'al'}\n",
    "\n",
    "# ==== Utils ====\n",
    "def coerce_to_str(x) -> str:\n",
    "    if isinstance(x, list):\n",
    "        x = \" \".join(map(str, x))\n",
    "    elif pd.isna(x):\n",
    "        x = \"\"\n",
    "    return str(x).strip()\n",
    "\n",
    "def safe_augment(aug, text: str, n: int = 1) -> List[str]:\n",
    "    \"\"\"Retourne une liste de n paraphrases (peut être < n si le modèle échoue).\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    try:\n",
    "        out = aug.augment(text, n=n)  # peut renvoyer str ou list[str]\n",
    "        if isinstance(out, str):\n",
    "            out = [out]\n",
    "        return [t.strip() for t in out if isinstance(t, str) and t.strip()]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def nettoyer_texte_tokens(texte: str) -> List[str]:\n",
    "    tokens = word_tokenize(texte)\n",
    "    tokens_nettoyes = []\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        token = re.sub(r'\\s+', '', token)\n",
    "        token = re.sub(r'[^a-zàâçéèêëîïôûùüÿñæœ]', '', token)\n",
    "        if token and token not in stop_words and token not in punctuation and token not in custom_stopwords:\n",
    "            token = lemmatizer.lemmatize(token)\n",
    "            tokens_nettoyes.append(token)\n",
    "    return tokens_nettoyes\n",
    "\n",
    "def tokens_to_text(tokens: List[str]) -> str:\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# ==== Device & models ====\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device choisi :\", device)\n",
    "\n",
    "from_model, to_model = (\n",
    "    (\"facebook/wmt19-en-de\", \"facebook/wmt19-de-en\") if device == \"cuda\"\n",
    "    else (\"Helsinki-NLP/opus-mt-en-de\", \"Helsinki-NLP/opus-mt-de-en\")\n",
    ")\n",
    "\n",
    "back_trans_aug = naw.BackTranslationAug(\n",
    "    from_model_name=from_model,\n",
    "    to_model_name=to_model,\n",
    "    device=device,\n",
    "    batch_size=32 if device == \"cuda\" else 8,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# ==== Load data ====\n",
    "df = pd.read_csv(\"./data/data_final_phase2_private.csv\")\n",
    "df[\"text\"] = df[\"text\"].apply(coerce_to_str)\n",
    "\n",
    "# (Re)crée text_clean / token_clean si manquants\n",
    "if \"token_clean\" not in df.columns:\n",
    "    df[\"token_clean\"] = df[\"text\"].apply(nettoyer_texte_tokens)\n",
    "else:\n",
    "    mask = df[\"token_clean\"].isna()\n",
    "    df.loc[mask, \"token_clean\"] = df.loc[mask, \"text\"].apply(nettoyer_texte_tokens)\n",
    "\n",
    "if \"text_clean\" not in df.columns:\n",
    "    df[\"text_clean\"] = df[\"token_clean\"].apply(tokens_to_text)\n",
    "else:\n",
    "    mask = df[\"text_clean\"].isna()\n",
    "    df.loc[mask, \"text_clean\"] = df.loc[mask, \"token_clean\"].apply(tokens_to_text)\n",
    "\n",
    "# ==== Filtrer uniquement la classe VS ====\n",
    "# (Robuste si tes labels sont 'VS'/'NVS' en str; adapte si 1/0)\n",
    "mask_vs = df[\"type_article\"].astype(str).str.upper().eq(\"VS\")\n",
    "df_vs = df[mask_vs].copy()\n",
    "\n",
    "# === Paramètre: nombre d'augmentations par article VS ===\n",
    "n_aug_per_sample = 1   # mets 2, 3… pour plus de paraphrases par texte VS\n",
    "\n",
    "aug_rows = []\n",
    "for _, row in df_vs.iterrows():\n",
    "    raw = row[\"text\"]\n",
    "    aug_texts = safe_augment(back_trans_aug, raw[:2000], n=n_aug_per_sample)\n",
    "    for aug_text in aug_texts:\n",
    "        tokens_bt = nettoyer_texte_tokens(aug_text)\n",
    "        aug_rows.append({\n",
    "            \"text_src\": raw,\n",
    "            \"text_final\": aug_text,\n",
    "            \"source\": \"bt\",\n",
    "            \"token_clean\": tokens_bt,\n",
    "            \"text_clean\": tokens_to_text(tokens_bt),\n",
    "            \"type_article\": row[\"type_article\"],\n",
    "            \"thematique\": row.get(\"thematique\", \"\")\n",
    "        })\n",
    "\n",
    "aug_df = pd.DataFrame(aug_rows)\n",
    "\n",
    "# ==== Bloc original (toutes les classes) ====\n",
    "train_original = df.copy()\n",
    "train_original[\"text_src\"] = train_original[\"text\"]\n",
    "train_original[\"text_final\"] = train_original[\"text\"]\n",
    "train_original[\"source\"] = \"orig\"\n",
    "\n",
    "# ==== Harmonisation & concat ====\n",
    "cols = [\"text_final\", \"text_clean\", \"token_clean\", \"source\", \"type_article\", \"thematique\", \"text_src\"]\n",
    "\n",
    "def ensure_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        return x.split()\n",
    "    return []\n",
    "\n",
    "train_original = train_original.reindex(columns=cols, fill_value=\"\")\n",
    "train_original[\"token_clean\"] = train_original[\"token_clean\"].apply(ensure_list)\n",
    "train_original[\"text_clean\"] = train_original[\"text_clean\"].astype(str)\n",
    "\n",
    "train_aug = aug_df.reindex(columns=cols, fill_value=\"\")\n",
    "\n",
    "train_data = pd.concat([train_original, train_aug], ignore_index=True)\n",
    "\n",
    "print(\"Nb lignes original :\", len(df))\n",
    "print(\"Nb VS augmentées   :\", len(aug_df))\n",
    "print(\"Taille finale       :\", train_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2af5880",
   "metadata": {},
   "source": [
    "#### Essayons ici de rattraper le nombre d'article de NVS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3f76dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device choisi : cuda\n",
      "NVS: 2241  VS: 249\n",
      "Chaque article VS doit produire à peu près 9 versions (dont l'original).\n",
      "Seulement 247 paraphrases uniques générées, < needed=1992.\n",
      "Articles VS générés (uniques) : 247\n",
      "Total VS (original + aug) : 496\n",
      "Total NVS : 2241\n",
      "Nb lignes original : 2490\n",
      "Nb VS augmentées   : 247\n",
      "Taille finale       : (2737, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "from typing import Optional, List\n",
    "import torch, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "import os\n",
    "import math\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# ==== Init NLTK stuff ====\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "custom_stopwords = {'et', 'al'}\n",
    "\n",
    "# ==== Utils ====\n",
    "def coerce_to_str(x) -> str:\n",
    "    if isinstance(x, list):\n",
    "        x = \" \".join(map(str, x))\n",
    "    elif pd.isna(x):\n",
    "        x = \"\"\n",
    "    return str(x).strip()\n",
    "\n",
    "def safe_augment(aug, text: str, n: int = 1) -> List[str]:\n",
    "    \"\"\"Retourne une liste de n paraphrases (peut être < n si le modèle échoue).\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    try:\n",
    "        out = aug.augment(text, n=n)  # peut renvoyer str ou list[str]\n",
    "        if isinstance(out, str):\n",
    "            out = [out]\n",
    "        return [t.strip() for t in out if isinstance(t, str) and t.strip()]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def nettoyer_texte_tokens(texte: str) -> List[str]:\n",
    "    tokens = word_tokenize(texte)\n",
    "    tokens_nettoyes = []\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        token = re.sub(r'\\s+', '', token)\n",
    "        token = re.sub(r'[^a-zàâçéèêëîïôûùüÿñæœ]', '', token)\n",
    "        if token and token not in stop_words and token not in punctuation and token not in custom_stopwords:\n",
    "            token = lemmatizer.lemmatize(token)\n",
    "            tokens_nettoyes.append(token)\n",
    "    return tokens_nettoyes\n",
    "\n",
    "def tokens_to_text(tokens: List[str]) -> str:\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# ==== Device & models ====\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device choisi :\", device)\n",
    "\n",
    "from_model, to_model = (\n",
    "    (\"facebook/wmt19-en-de\", \"facebook/wmt19-de-en\") if device == \"cuda\"\n",
    "    else (\"Helsinki-NLP/opus-mt-en-de\", \"Helsinki-NLP/opus-mt-de-en\")\n",
    ")\n",
    "\n",
    "back_trans_aug = naw.BackTranslationAug(\n",
    "    from_model_name=from_model,\n",
    "    to_model_name=to_model,\n",
    "    device=device,\n",
    "    batch_size=32 if device == \"cuda\" else 8,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# ==== Load data ====\n",
    "df = pd.read_csv(\"./data/data_final_phase2_private.csv\")\n",
    "df[\"text\"] = df[\"text\"].apply(coerce_to_str)\n",
    "\n",
    "# (Re)crée text_clean / token_clean si manquants\n",
    "if \"token_clean\" not in df.columns:\n",
    "    df[\"token_clean\"] = df[\"text\"].apply(nettoyer_texte_tokens)\n",
    "else:\n",
    "    mask = df[\"token_clean\"].isna()\n",
    "    df.loc[mask, \"token_clean\"] = df.loc[mask, \"text\"].apply(nettoyer_texte_tokens)\n",
    "\n",
    "if \"text_clean\" not in df.columns:\n",
    "    df[\"text_clean\"] = df[\"token_clean\"].apply(tokens_to_text)\n",
    "else:\n",
    "    mask = df[\"text_clean\"].isna()\n",
    "    df.loc[mask, \"text_clean\"] = df.loc[mask, \"token_clean\"].apply(tokens_to_text)\n",
    "\n",
    "# ==== Filtrer uniquement la classe VS ====\n",
    "\n",
    "mask_vs = df[\"type_article\"].astype(str).str.upper().eq(\"VS\")\n",
    "df_vs = df[mask_vs].copy()\n",
    "\n",
    "\n",
    "\n",
    "# --- Comptage ---\n",
    "count_vs  = (df[\"type_article\"].astype(str).str.upper() == \"VS\").sum()\n",
    "count_nvs = (df[\"type_article\"].astype(str).str.upper() == \"NVS\").sum()\n",
    "print(\"NVS:\", count_nvs, \" VS:\", count_vs)\n",
    "\n",
    "target = count_nvs\n",
    "needed = max(0, target - count_vs)\n",
    "if needed == 0:\n",
    "    print(\"Pas besoin d'augmentation: VS est déjà ≥ NVS.\")\n",
    "\n",
    "# --- Génération ---\n",
    "aug_rows = []\n",
    "if needed > 0:\n",
    "    factor = math.ceil(target / count_vs)  # nb total de versions par article (original compris)\n",
    "    print(f\"Chaque article VS doit produire à peu près {factor} versions (dont l'original).\")\n",
    "\n",
    "    for _, row in df_vs.iterrows():\n",
    "        raw = row[\"text\"]\n",
    "        aug_texts = safe_augment(back_trans_aug, raw[:2000], n=max(1, factor-1))\n",
    "        # construire les lignes\n",
    "        for aug_text in aug_texts:\n",
    "            tokens_bt = nettoyer_texte_tokens(aug_text)\n",
    "            aug_rows.append({\n",
    "                \"text_src\": raw,\n",
    "                \"text_final\": aug_text,\n",
    "                \"source\": \"bt\",\n",
    "                \"token_clean\": tokens_bt,\n",
    "                \"text_clean\": tokens_to_text(tokens_bt),\n",
    "                \"type_article\": row[\"type_article\"],\n",
    "                \"thematique\": row.get(\"thematique\", \"\")\n",
    "            })\n",
    "\n",
    "# DataFrame des augmentées\n",
    "aug_df = pd.DataFrame(aug_rows)\n",
    "\n",
    "# --- Déduplication robuste ---\n",
    "# 1) enlever les lignes vides/NaN\n",
    "aug_df = aug_df[aug_df[\"text_final\"].astype(str).str.strip().ne(\"\")].dropna(subset=[\"text_final\"])\n",
    "\n",
    "# 2) dédupliquer par paraphrase (et étiquette) pour éviter redites exactes\n",
    "aug_df = aug_df.drop_duplicates(subset=[\"text_final\", \"type_article\"], keep=\"first\")\n",
    "\n",
    "# 3) si trop de lignes, échantillonner pour viser exactement \"needed\"\n",
    "if len(aug_df) > needed:\n",
    "    # Utilisons random_sate pour controler l'aléa \n",
    "    aug_df = aug_df.sample(n=needed, random_state=42).reset_index(drop=True)\n",
    "elif len(aug_df) < needed:\n",
    "    print(f\"Seulement {len(aug_df)} paraphrases uniques générées, < needed={needed}.\")\n",
    "  \n",
    "\n",
    "print(\"Articles VS générés (uniques) :\", len(aug_df))\n",
    "print(\"Total VS (original + aug) :\", count_vs + len(aug_df))\n",
    "print(\"Total NVS :\", count_nvs)\n",
    "\n",
    "# --- Bloc original & concat finale ---\n",
    "train_original = df.copy()\n",
    "train_original[\"text_src\"]   = train_original[\"text\"]\n",
    "train_original[\"text_final\"] = train_original[\"text\"]\n",
    "train_original[\"source\"]     = \"orig\"\n",
    "\n",
    "cols = [\"text_final\", \"text_clean\", \"token_clean\", \"source\", \"type_article\", \"thematique\", \"text_src\"]\n",
    "\n",
    "def ensure_list(x):\n",
    "    if isinstance(x, list): return x\n",
    "    if isinstance(x, str):  return x.split()\n",
    "    return []\n",
    "\n",
    "train_original = train_original.reindex(columns=cols, fill_value=\"\")\n",
    "train_original[\"token_clean\"] = train_original[\"token_clean\"].apply(ensure_list)\n",
    "train_original[\"text_clean\"]  = train_original[\"text_clean\"].astype(str)\n",
    "\n",
    "train_aug = aug_df.reindex(columns=cols, fill_value=\"\")\n",
    "\n",
    "train_data = pd.concat([train_original, train_aug], ignore_index=True)\n",
    "print(\"Nb lignes original :\", len(df))\n",
    "print(\"Nb VS augmentées   :\", len(aug_df))\n",
    "print(\"Taille finale       :\", train_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2551be81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_final</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>token_clean</th>\n",
       "      <th>source</th>\n",
       "      <th>type_article</th>\n",
       "      <th>thematique</th>\n",
       "      <th>text_src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Microbial Community Composition Associated wit...</td>\n",
       "      <td>microbial community composition associated pot...</td>\n",
       "      <td>[microbial, community, composition, associated...</td>\n",
       "      <td>orig</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "      <td>Microbial Community Composition Associated wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Plant Pathogenic and Endophytic Colletotrichum...</td>\n",
       "      <td>plant pathogenic endophytic colletotrichum fru...</td>\n",
       "      <td>[plant, pathogenic, endophytic, colletotrichum...</td>\n",
       "      <td>orig</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "      <td>Plant Pathogenic and Endophytic Colletotrichum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lethal Bronzing: What you should know about th...</td>\n",
       "      <td>lethal bronzing know disease turn palm tree br...</td>\n",
       "      <td>[lethal, bronzing, know, disease, turn, palm, ...</td>\n",
       "      <td>orig</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "      <td>Lethal Bronzing: What you should know about th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leaffooted Bug Damage in Almond Orchards Leaff...</td>\n",
       "      <td>leaffooted bug damage almond orchard leaffoote...</td>\n",
       "      <td>[leaffooted, bug, damage, almond, orchard, lea...</td>\n",
       "      <td>orig</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "      <td>Leaffooted Bug Damage in Almond Orchards Leaff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kebbi govt battles mysterious disease affectin...</td>\n",
       "      <td>kebbi govt battle mysterious disease affecting...</td>\n",
       "      <td>[kebbi, govt, battle, mysterious, disease, aff...</td>\n",
       "      <td>orig</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "      <td>Kebbi govt battles mysterious disease affectin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>Mystery Seed Packages Appearing Once Again in ...</td>\n",
       "      <td>mystery seed package appearing alabama mystery...</td>\n",
       "      <td>[mystery, seed, package, appearing, alabama, m...</td>\n",
       "      <td>bt</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "      <td>Mystery Seed Packages Appearing Once Again in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2733</th>\n",
       "      <td>ACES: Mystery seed packages appeared again in ...</td>\n",
       "      <td>ace mystery seed package appeared alabama ace ...</td>\n",
       "      <td>[ace, mystery, seed, package, appeared, alabam...</td>\n",
       "      <td>bt</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "      <td>ACES: Mystery seed packages appearing once aga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2734</th>\n",
       "      <td>Farmers blame the plague: 150,000 per bag Farm...</td>\n",
       "      <td>farmer blame plague per bag farmer blame plagu...</td>\n",
       "      <td>[farmer, blame, plague, per, bag, farmer, blam...</td>\n",
       "      <td>bt</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "      <td>Farmers Blame Unknown Pest As Pepper Hits ₦150...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2735</th>\n",
       "      <td>Sharp drop in yield due to mysterious fungal i...</td>\n",
       "      <td>sharp drop yield due mysterious fungal infecti...</td>\n",
       "      <td>[sharp, drop, yield, due, mysterious, fungal, ...</td>\n",
       "      <td>bt</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "      <td>Sharp decline in yield as mysterious fungal in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736</th>\n",
       "      <td>According to Prairie Crop Disease Disease Moni...</td>\n",
       "      <td>according prairie crop disease disease monitor...</td>\n",
       "      <td>[according, prairie, crop, disease, disease, m...</td>\n",
       "      <td>bt</td>\n",
       "      <td>VS</td>\n",
       "      <td>SV</td>\n",
       "      <td>Physiological leaf spot suspected in southern ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2737 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_final  \\\n",
       "0     Microbial Community Composition Associated wit...   \n",
       "1     Plant Pathogenic and Endophytic Colletotrichum...   \n",
       "2     Lethal Bronzing: What you should know about th...   \n",
       "3     Leaffooted Bug Damage in Almond Orchards Leaff...   \n",
       "4     Kebbi govt battles mysterious disease affectin...   \n",
       "...                                                 ...   \n",
       "2732  Mystery Seed Packages Appearing Once Again in ...   \n",
       "2733  ACES: Mystery seed packages appeared again in ...   \n",
       "2734  Farmers blame the plague: 150,000 per bag Farm...   \n",
       "2735  Sharp drop in yield due to mysterious fungal i...   \n",
       "2736  According to Prairie Crop Disease Disease Moni...   \n",
       "\n",
       "                                             text_clean  \\\n",
       "0     microbial community composition associated pot...   \n",
       "1     plant pathogenic endophytic colletotrichum fru...   \n",
       "2     lethal bronzing know disease turn palm tree br...   \n",
       "3     leaffooted bug damage almond orchard leaffoote...   \n",
       "4     kebbi govt battle mysterious disease affecting...   \n",
       "...                                                 ...   \n",
       "2732  mystery seed package appearing alabama mystery...   \n",
       "2733  ace mystery seed package appeared alabama ace ...   \n",
       "2734  farmer blame plague per bag farmer blame plagu...   \n",
       "2735  sharp drop yield due mysterious fungal infecti...   \n",
       "2736  according prairie crop disease disease monitor...   \n",
       "\n",
       "                                            token_clean source type_article  \\\n",
       "0     [microbial, community, composition, associated...   orig           VS   \n",
       "1     [plant, pathogenic, endophytic, colletotrichum...   orig           VS   \n",
       "2     [lethal, bronzing, know, disease, turn, palm, ...   orig           VS   \n",
       "3     [leaffooted, bug, damage, almond, orchard, lea...   orig           VS   \n",
       "4     [kebbi, govt, battle, mysterious, disease, aff...   orig           VS   \n",
       "...                                                 ...    ...          ...   \n",
       "2732  [mystery, seed, package, appearing, alabama, m...     bt           VS   \n",
       "2733  [ace, mystery, seed, package, appeared, alabam...     bt           VS   \n",
       "2734  [farmer, blame, plague, per, bag, farmer, blam...     bt           VS   \n",
       "2735  [sharp, drop, yield, due, mysterious, fungal, ...     bt           VS   \n",
       "2736  [according, prairie, crop, disease, disease, m...     bt           VS   \n",
       "\n",
       "     thematique                                           text_src  \n",
       "0            SV  Microbial Community Composition Associated wit...  \n",
       "1            SV  Plant Pathogenic and Endophytic Colletotrichum...  \n",
       "2            SV  Lethal Bronzing: What you should know about th...  \n",
       "3            SV  Leaffooted Bug Damage in Almond Orchards Leaff...  \n",
       "4            SV  Kebbi govt battles mysterious disease affectin...  \n",
       "...         ...                                                ...  \n",
       "2732         SV  Mystery Seed Packages Appearing Once Again in ...  \n",
       "2733         SV  ACES: Mystery seed packages appearing once aga...  \n",
       "2734         SV  Farmers Blame Unknown Pest As Pepper Hits ₦150...  \n",
       "2735         SV  Sharp decline in yield as mysterious fungal in...  \n",
       "2736         SV  Physiological leaf spot suspected in southern ...  \n",
       "\n",
       "[2737 rows x 7 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a58f4b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"text_final\"].iloc[2735] == train_data[\"text_src\"].iloc[2735]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7966f22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type_article\n",
       "NVS    2241\n",
       "VS      496\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"type_article\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5dd94dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type_article\n",
       "NVS    2241\n",
       "VS      249\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"type_article\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5284e3e",
   "metadata": {},
   "source": [
    "#### Les textes augmentées par la rétro traduction ont une source bt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8a46c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sauvegardons les données augmentées dans un fichier csv \n",
    "\n",
    "train_data.to_csv(\"./data/data_augmented_back_traduction.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c982ca27",
   "metadata": {},
   "source": [
    "###  Nous allons reprendre la classification avec le fine-tuning de SBERT que nous avons fait à la phase 3 \n",
    "\n",
    "voir le fichier notebook *fine_tuning_experiments_augmentated.ipynb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4058528",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/data_augmented_back_traduction.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
